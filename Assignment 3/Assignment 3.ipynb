{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fe68c4",
   "metadata": {},
   "source": [
    "# Changelog\n",
    "1. Added problem description to introduction\n",
    "2. Changed all variable names for consistency\n",
    "3. Merged some code of multiple cells to a single cell\n",
    "4. Added 1.3 limitations dataset\n",
    "5. Changed 2.2 cutoff values 0.5 -> 0.2, 0.5 -> 0.65\n",
    "6. Changed 2.2 conclusion for new cutoff values\n",
    "7. Moved 2.4 alternative method down to avoid confusion\n",
    "8. Changed 2.4 conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82a8b8",
   "metadata": {},
   "source": [
    "# SEN163A - Fundamentals of Data Analytics\n",
    "# Assignment 3 - Large-scale Internet Data Analysis\n",
    "### Ir. Jacopo De Stefani - [J.deStefani@tudelft.nl](mailto:J.deStefani@tudelft.nl)\n",
    "### Joao Pizani Flor, M.Sc. - [J.p.pizaniflor@tudelft.nl](mailto:J.p.pizaniflor@tudelft.nl)\n",
    "\n",
    "### 15-04-2022\n",
    "## Group 2\n",
    "- Emmanuel M Boateng - '5617642'\n",
    "- Joost Oortwijn - '4593472'\n",
    "- Philip Busscher - ''4611993''\n",
    "- Floris Kool - ''4975243''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7fabaf",
   "metadata": {},
   "source": [
    "# Introduction & problem description\n",
    "Tabularazor inc. is a large national newspaper covering many different topics. The company suffered an electrical malfunction causing a loss of archived data. Luckily they have a backup prepared of the articles published over the last couple of years. The backup from Tabularazor inc. contains the articles sorted by month and year, including the authors of the articles and the publishing day and time. In this assignment we will use this metadata to deduce some interesting facts about Tabularazors employees. We want to answer the following questions:<br>\n",
    "\n",
    "a. Are there couples among the employees? <br>\n",
    "b. Did any of the employees have childeren?<br>\n",
    "c. How many holidays does an employee have?<br>\n",
    "\n",
    "The problem with answering these questions is the fact that there is only metadata available. In other words, the available data does not not directly relate to these questions. But, based on our own interpretation of the available metadata, we can answer these questions. This interpretation will be eleborated on below in chapter 2.\n",
    "\n",
    "We will start by describing how the data is retrieved from the backup using webscraping and describing the available data in chapter 1. After this, in chapter 2, the data is analyzed and conclusions will be drawn which are usefull for answering the above named questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec28dd7",
   "metadata": {},
   "source": [
    "# 1. Dataset description\n",
    "\n",
    "The provided dataset contains an archief of a newpaper company with publications from to year 2012 till 2019. Per article, the publication date, publication time and the responsible author are included within the dataset. Based on these three attributes further analysis will be conducted.\n",
    "\n",
    "The data can be found on a website (https://jdestefani.github.io/SEN163A-TabularRazorArchives/). That means that the data is stored on a webserver and a specific approach is required to obtain the data in the form of an HTML file. This process is called webscraping. To enable webscraping the python library Beautifulsoup is selected. This library contains the tools to work with HTML files like the provided data. Though, to obtain the data from the webserver, another library is necessary to generate HTTP requests. This library is called: requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c52b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d50ec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run this to allign the tables correctly\n",
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcd1f9",
   "metadata": {},
   "source": [
    "### Globally used variable names\n",
    "complete_df - Dataframe with the author name, date and time for every article\n",
    "\n",
    "employees_df - Dataframe with for each author and each day a field containing:\n",
    "\n",
    "| Value in dataframe     | Meaning |\n",
    "| ----------- | ----------- |\n",
    "| 0      | not_employed       |\n",
    "| 1   | Working        |\n",
    "| 2   | Offday        |\n",
    "| 3   | weekend_Working        |\n",
    "| 4   | weekend_Offday        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac22f0",
   "metadata": {},
   "source": [
    "potential_couples - List of all possible pairs of two authors\n",
    "\n",
    "couples_df - Dataframe with the following information for each author:\n",
    " - Number of days at least one of the couple wasn't employed\n",
    " - Number of days both people were working\n",
    " - Number of days only one of the couple was working\n",
    " - Number of days neither people were working during a weekend\n",
    " - Number of days neither people were working on a workday\n",
    " - ratio of days worked together over days that one person was working\n",
    " - ratio of days they were off together over days that only one person was off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4032",
   "metadata": {},
   "source": [
    "## 1.1 Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37564b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING MAIN PAGE\n",
    "#Get file\n",
    "source_link = \"https://jdestefani.github.io/SEN163A-TabularRazorArchives/\"\n",
    "response = requests.get(source_link)\n",
    "\n",
    "#Read main file\n",
    "main_page = BeautifulSoup(response.text)\n",
    "\n",
    "for year_link in main_page.find_all('a', href=True):\n",
    "    #READING YEAR PAGE & SAVING PKL FILE FOR EACH YEAR\n",
    "    \n",
    "    #Get file name of pkl file\n",
    "    year_filename = year_link.get('href')\n",
    "    year_filename = year_filename.removeprefix('./')\n",
    "    year_filename = year_filename.removesuffix('.html')\n",
    "    year_filename = \"data/\" + year_filename + \".pkl\"\n",
    "    \n",
    "    #Clear lists of data for each year\n",
    "    data_tuple = []\n",
    "    \n",
    "    #Get file  \n",
    "    year_link = urljoin(source_link, year_link.get('href')) \n",
    "    response = requests.get(year_link)\n",
    "    \n",
    "    #Read year file\n",
    "    year_page = BeautifulSoup(response.text)\n",
    "     \n",
    "    for month_link in year_page.find_all('a', href=True):\n",
    "        #Get file\n",
    "        month_link = urljoin(year_link, month_link.get('href'))\n",
    "        response = requests.get(month_link)\n",
    "        \n",
    "        #Read month file\n",
    "        month_page = BeautifulSoup(response.text)\n",
    "        i = 0\n",
    "        \n",
    "        start  = time.time()\n",
    "        \n",
    "        for article_link in month_page.find_all('a', href=True):        \n",
    "            #Get file\n",
    "            article_link = urljoin(month_link, article_link.get('href')) \n",
    "            response = requests.get(article_link)\n",
    "        \n",
    "            #Read article file\n",
    "            article_page = BeautifulSoup(response.text)\n",
    "                       \n",
    "            for div_element in article_page.find_all('div'):\n",
    "                if div_element.get('class') == ['author']:\n",
    "                    article_author = div_element.get_text()\n",
    "                if div_element.get('class') == ['date']:\n",
    "                    article_date = div_element.get_text()\n",
    "                if div_element.get('class') == ['time']:\n",
    "                    article_time = div_element.get_text()\n",
    "\n",
    "            #Add data\n",
    "            data_tuple.append((article_author, article_date, article_time))\n",
    "            i += 1 \n",
    "            #if i >= 10:\n",
    "            #    break\n",
    "        \n",
    "        #Print after each month\n",
    "        dur = round(time.time() - start,2)\n",
    "        print('Read: ' + month_link)\n",
    "        print('Added: ' + str(i) + ' articles')\n",
    "        print('Duration: ' + str(dur) + ' seconds')\n",
    "        \n",
    "        #break\n",
    "\n",
    "    #Save file after each year\n",
    "    print()\n",
    "    print(\"---\")\n",
    "    print(\"Saving: \" + year_filename)\n",
    "    df = pd.DataFrame(data_tuple)\n",
    "    df.to_pickle(year_filename)\n",
    "    print(\"Saved: \" + str(len(df[0])) + \" articles\")\n",
    "    print(\"---\")\n",
    "    print()\n",
    "    \n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385a0d9",
   "metadata": {},
   "source": [
    "## 1.2 Opening the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c9d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_df = pickle.load(open(\"2012_2019.pkl\", \"rb\")) # use this as examiner\n",
    "complete_df = pickle.load(open(\"data/2012_2019.pkl\", \"rb\"))\n",
    "complete_df = complete_df.reset_index()\n",
    "complete_df = complete_df.drop(['index'], axis=1) \n",
    "complete_df.columns = ['Author','Date','Time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a252469",
   "metadata": {},
   "source": [
    "## 1.3 Limitations dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c54606",
   "metadata": {},
   "source": [
    "Beforehand we must check if the dataset is clean and useable for the analysis. Therefore we check if the columns of interest contain the right value types and we check if there are NaN values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd62048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are missing errors in the dataset\n",
    "complete_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2290f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if only date times in column \"date\"\n",
    "complete_df['Date'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "576b9eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if only strings in column \"author\"\n",
    "complete_df['Author'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467009c0",
   "metadata": {},
   "source": [
    "Based on the check above we found no missing data in the dataset which could influence the analysis. All values had the expected datatype suitable for analyzing and also no NaN values are found in the dataset. \n",
    "\n",
    "The major limitations of this analysis is that we are using metadata of when an author publishes to make conclusions about when they are working. These limitations are described at the anwsers of each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf6846",
   "metadata": {},
   "source": [
    "# 2 Analysis\n",
    "\n",
    "The following analysis focuses on investigating how colleagues at Tubularazor are related to each other. By looking at the author names and publication dates of the articles, certain patterns can be observed. These patterns are able to clearify what kind of relations the employees have with respect to each other. We will look to find couples within Tubularazor authors, whether an author has had extended leave due to possible child birth and the average amount of holidays for employees of Tubularazor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29d4b6",
   "metadata": {},
   "source": [
    "## 2.1 Tranform data\n",
    "\n",
    "We will start by transforming the raw data into a dataset useful for our analysis questions. For our analysis we only need to know for each author on what day they have posted an article. This way we can deduce on what days people were working, which will be used in the questions.\n",
    "\n",
    "After doing this we can add some extra information. An author might not be employed throughout the entire period that we're analyzing. We're using the date they first and last posted to deduce when someone might have started or stopped working for Trebularazor. We also noticed that there are significantly less articles posted in the weekend. We also want to include this information. The result is a dataset with a field for each author on each day that can have the following value:\n",
    "- Not employed\n",
    "- Working\n",
    "- Day off\n",
    "- Working on weekend\n",
    "- weekend day off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b94b1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all the publication dates per author\n",
    "merged_groupby = complete_df.groupby(['Author'])['Date'].unique().apply(list).reset_index()\n",
    "merged_groupby = merged_groupby.set_index('Author')\n",
    "\n",
    "# Split the list with publication dates to a column\n",
    "split_df = pd.DataFrame(merged_groupby['Date'].tolist())\n",
    "split_df = split_df.set_index(merged_groupby.index)\n",
    "split_df = split_df.astype(str)\n",
    "\n",
    "# Create range of dates from the beginning of 2012 till the end of 20190\n",
    "start = datetime.strptime(\"2012-1-1\", \"%Y-%m-%d\")\n",
    "end = datetime.strptime(\"2019-12-31\", \"%Y-%m-%d\")\n",
    "date_generated = pd.date_range(start, end)\n",
    "\n",
    "# Create new dataframe with only zero's \n",
    "zero_df = pd.DataFrame(np.zeros((len(split_df), len(date_generated)), np.uint8))\n",
    "zero_df = zero_df.set_index(merged_groupby.index)\n",
    "zero_df.columns = date_generated\n",
    "zero_df.columns = zero_df.columns.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Fill the new dataframe (with zero's) with ones in case an author has published on a specific date\n",
    "list_index = list(split_df.index.values)\n",
    "\n",
    "for i in list_index:\n",
    "    for j in range(split_df.shape[1]):\n",
    "        date = split_df.loc[i,j]\n",
    "        if np.isnat(np.datetime64(date))==True:\n",
    "            break\n",
    "        else:\n",
    "            zero_df.loc[i,date]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef99c5f",
   "metadata": {},
   "source": [
    "Creating a dataframe called employees_df with all employees and all days containing:\n",
    "\n",
    "\n",
    "| Value in dataframe     | Meaning |\n",
    "| ----------- | ----------- |\n",
    "| 0      | not_employed       |\n",
    "| 1   | Working        |\n",
    "| 2   | Offday        |\n",
    "| 3   | weekend_Working        |\n",
    "| 4   | weekend_Offday        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c269a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "#Fill with not_employed until their first publication\n",
    "#Fill with workdays and offdays based on publication\n",
    "employees_data = []\n",
    "\n",
    "for employee in zero_df.iterrows():\n",
    "    days = []\n",
    "    startedworking = False\n",
    "    \n",
    "    for day in employee[1]:\n",
    "        if day == 1:\n",
    "            startedworking = True\n",
    "            days.append(1) #Working\n",
    "        else:\n",
    "            if startedworking:\n",
    "                days.append(2) #Offday\n",
    "            else:\n",
    "                days.append(0) #not_employed\n",
    "    \n",
    "    employees_data.append(days)\n",
    "                \n",
    "employees_df = pd.DataFrame(employees_data, columns = date_generated.strftime(\"%Y-%m-%d\"))\n",
    "employees_df.index = zero_df.index\n",
    "\n",
    "\n",
    "#Fill not employed based on last publication day\n",
    "rowindex = 0\n",
    "\n",
    "for employee in employees_df.iterrows():    \n",
    "    \n",
    "    #Run through the days reversed to find last publication day\n",
    "    columnindex = len(employee[1])\n",
    "    \n",
    "    for day in reversed(employee[1]): \n",
    "        columnindex -= 1\n",
    "\n",
    "        if day == 1:\n",
    "            break\n",
    "        else:        \n",
    "            employees_df.iat[rowindex,columnindex] = 0 #not_employed    \n",
    "            \n",
    "    rowindex += 1\n",
    "    \n",
    "# Change weekend days to right lables\n",
    "from datetime import datetime\n",
    "for i in list(employees_df.index.values):\n",
    "    for j in list(employees_df.columns):\n",
    "        year = int(j[:4])\n",
    "        month = int(j[5:7])\n",
    "        day = int(j[-2:])\n",
    "        date = datetime(year,month,day)\n",
    "        if date.weekday() > 4:\n",
    "            if employees_df.loc[i,j] == 1:\n",
    "                employees_df.loc[i,j] = 3 #weekend_Working\n",
    "            elif employees_df.loc[i,j] == 2:\n",
    "                employees_df.loc[i,j] = 4 #weekend_Offday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d258127",
   "metadata": {},
   "source": [
    "## 2.2 Couple among employees (Question a)\n",
    "\n",
    "The first test that will be conducted is whether there are couples among the authors and if they are still together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988fa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of all potential couples\n",
    "authors = set(employees_df.index)\n",
    "potential_couples =  [couple for couple in combinations(authors,2)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615741fb",
   "metadata": {},
   "source": [
    "Creating a dataframe with all couple of employees containing:\n",
    " - Number of days at least one of the couple wasn't employed\n",
    " - Number of days both people were working\n",
    " - Number of days only one of the couple was working\n",
    " - Number of days neither people were working during a weekend\n",
    " - Number of days neither people were working on a workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798c438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading took: 136.15 seconds\n"
     ]
    }
   ],
   "source": [
    "#LOADING WILL TAKE SOME TIME (2-3 minutes)\n",
    "\n",
    "not_employed = []\n",
    "both_working = []\n",
    "one_working = []\n",
    "weekend = []\n",
    "offdays = []\n",
    "couples_data = []\n",
    "\n",
    "start  = time.time()\n",
    "\n",
    "#Check for all couples\n",
    "for potential_couple in potential_couples:\n",
    "    \n",
    "    #Reset counters for each couple\n",
    "    not_employed_count = 0\n",
    "    both_working_count = 0\n",
    "    one_working_count = 0\n",
    "    weekend_count = 0\n",
    "    offdays_count = 0\n",
    "    templist = []\n",
    "    \n",
    "    #Select couple\n",
    "    temp_df = employees_df.loc[[potential_couple[0],potential_couple[1]]]\n",
    "    \n",
    "    for column in temp_df:\n",
    "        \n",
    "        #Checking both employees for:\n",
    "        #0 - not_employed\n",
    "        #1 - Working\n",
    "        #2 - Offday\n",
    "        #3 - weekend_Working\n",
    "        #4 - weekend_Offday\n",
    "        \n",
    "        #Add 1 to counter based on conditions mentioned in cell above\n",
    "        if temp_df[column][0] == 0 or temp_df[column][1] == 0:\n",
    "            not_employed_count += 1\n",
    "        elif (temp_df[column][0] == 1 and temp_df[column][1] == 1) or (temp_df[column][0] == 3 and temp_df[column][1] == 3):\n",
    "            both_working_count += 1\n",
    "        elif (temp_df[column][0] == 1 or temp_df[column][1] == 1) or (temp_df[column][0] == 3 or temp_df[column][1] == 3):\n",
    "            one_working_count += 1\n",
    "        elif temp_df[column][0] == 4 and temp_df[column][1] == 4:\n",
    "            weekend_count += 1\n",
    "        else:\n",
    "            offdays_count += 1\n",
    "    \n",
    "    #Add employee count data to tuple of lists\n",
    "    templist.append(not_employed_count)\n",
    "    templist.append(both_working_count)\n",
    "    templist.append(one_working_count)\n",
    "    templist.append(weekend_count)\n",
    "    templist.append(offdays_count)           \n",
    "    couples_data.append((templist))\n",
    "\n",
    "#Create dataframe of all couples      \n",
    "couples_df = pd.DataFrame(couples_data, columns = ['Not employed', 'Both working', 'One working', 'weekend', 'Off days'])\n",
    "couples_df.index = potential_couples[:len(couples_data)]\n",
    "\n",
    "dur = round(time.time() - start,2)\n",
    "print(\"Loading took: \" + str(dur) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d1905",
   "metadata": {},
   "source": [
    "To see if two people are a couple we assume they often work together and often take days off together. For all potential couples we will calculate two ratio's:\n",
    "1. Days worked togeter over days that at least one person was working\n",
    "2. Off days together over days that at least one person was not working\n",
    "\n",
    "After some testing we noticed significant influences of days that people were unemployed and weekend days. If people were both not working for Trebulazor they had many \"days off\" together, by including the weekends we also had higher ratio's of people having many \"days off\" together. This is why we categorized these values in 2.1 and filtered for them in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d66fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freetogether_ratio = []\n",
    "worktogether_ratio = []\n",
    "\n",
    "#Calculate ratio's for each couple by:\n",
    "#Free days/ (Free days + One person working days)\n",
    "#Both people working days/ (Both people working days + One person working days)\n",
    "\n",
    "for potential_couple in couples_df.iterrows():\n",
    "    \n",
    "    #Added a divide by 0 check\n",
    "    if potential_couple[1]['Off days'] + potential_couple[1]['One working'] != 0:\n",
    "        freetogether_ratio.append(potential_couple[1]['Off days']/(potential_couple[1]['Off days'] + potential_couple[1]['One working']))\n",
    "    else:\n",
    "        freetogether_ratio.append(0)\n",
    "    \n",
    "    #Added a divide by 0 check\n",
    "    if potential_couple[1]['Both working'] + potential_couple[1]['One working'] != 0:\n",
    "        worktogether_ratio.append(potential_couple[1]['Both working']/(potential_couple[1]['Both working'] + potential_couple[1]['One working']))\n",
    "    else:\n",
    "        worktogether_ratio.append(0)\n",
    "\n",
    "couples_df['free_ratio'] = freetogether_ratio\n",
    "couples_df['work_ratio'] = worktogether_ratio\n",
    "\n",
    "#couples_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebeefee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Ratio'),\n",
       " Text(0, 0.5, '=Frequency'),\n",
       " Text(0.5, 1.0, 'Work days ratio')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the distribution of ratio's to help find couples\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "axs[0].hist(couples_df['free_ratio'], bins=20)\n",
    "axs[1].hist(couples_df['work_ratio'], bins=20)\n",
    "\n",
    "axs[0].set(xlabel='Ratio', ylabel='=Frequency',\n",
    "       title='Free days ratio')\n",
    "axs[1].set(xlabel='Ratio', ylabel='=Frequency',\n",
    "       title='Work days ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc271052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 couples found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Not employed</th>\n",
       "      <th>Both working</th>\n",
       "      <th>One working</th>\n",
       "      <th>weekend</th>\n",
       "      <th>Off days</th>\n",
       "      <th>free_ratio</th>\n",
       "      <th>work_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(Augusta Beltrami, Grover Gibbons)</th>\n",
       "      <td>1827</td>\n",
       "      <td>695</td>\n",
       "      <td>44</td>\n",
       "      <td>268</td>\n",
       "      <td>88</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.940460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Corrine Gallop, Randall Feather)</th>\n",
       "      <td>1552</td>\n",
       "      <td>611</td>\n",
       "      <td>281</td>\n",
       "      <td>355</td>\n",
       "      <td>123</td>\n",
       "      <td>0.304455</td>\n",
       "      <td>0.684978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Julieta Knapp, Vonk Billips)</th>\n",
       "      <td>1</td>\n",
       "      <td>1849</td>\n",
       "      <td>211</td>\n",
       "      <td>739</td>\n",
       "      <td>122</td>\n",
       "      <td>0.366366</td>\n",
       "      <td>0.897573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Not employed  Both working  One working  \\\n",
       "(Augusta Beltrami, Grover Gibbons)          1827           695           44   \n",
       "(Corrine Gallop, Randall Feather)           1552           611          281   \n",
       "(Julieta Knapp, Vonk Billips)                  1          1849          211   \n",
       "\n",
       "                                    weekend  Off days  free_ratio  work_ratio  \n",
       "(Augusta Beltrami, Grover Gibbons)      268        88    0.666667    0.940460  \n",
       "(Corrine Gallop, Randall Feather)       355       123    0.304455    0.684978  \n",
       "(Julieta Knapp, Vonk Billips)           739       122    0.366366    0.897573  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couples = []\n",
    "\n",
    "#Cutoff values:\n",
    "free_ratio_cutoff = 0.2\n",
    "work_ratio_cutoff = 0.65\n",
    "\n",
    "#Add couples with ratio's higher than the cutoff values\n",
    "for potential_couple in couples_df.iterrows():\n",
    "    if potential_couple[1]['free_ratio'] >= free_ratio_cutoff and potential_couple[1]['work_ratio'] >= work_ratio_cutoff:\n",
    "        couples.append(potential_couple[0])\n",
    "\n",
    "print(str(len(couples)) + \" couples found\")\n",
    "\n",
    "selected_couples_df = couples_df.loc[couples]\n",
    "\n",
    "selected_couples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18d9a3",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "To check which pairs of authors might be a couple we use the ratio's described earlier. Using the cutoff values (in the cell above) we can set different thresholds for what a couple is and how often they would need to work together or take days off together. Before removing the weekend and not employed days we noticed many potential couples and changing the cutoff values changed the outcomes. \n",
    "\n",
    "After removing the weekend and not employed days however we only noticed one clear outlier: Grover Gibbons & Augusta Beltrami. Out of the 739 days they were working 695 days were worked together (94%) and out of 132 holidays 88 were at the same time (67%). When lowering the cutoff values we identified two other couples: Vonk Billips & Julieta Knapp and Randall Feather and Corrine Gallop. These couples did only take 37% and 30% percent of their off days together, so it is hard to say with certainty if these are actual couples. \n",
    "\n",
    "Lowering the cutoff values further resulted in many more couples being identified, with some authors being in multiple couples. Therefor we think that 20% free days ratio and 65% work days ratio are correct cutoff values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1258f",
   "metadata": {},
   "source": [
    "## 2.3 Children (Question b)\n",
    "*Did any of the employees have a child? If so, who?*\n",
    "\n",
    "To deduce whether someone had a child we can use the number of days that people were not publishing for Tabularazor. Though we can only speculate why a person has a long period between publishing we will asume that if a person is away for longer than 80 days she is away for pregnancy leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7825412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Max_Absence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Corrine Gallop</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Marthe Hale</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Julieta Knapp</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Augusta Beltrami</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Grover Gibbons</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author  Max_Absence\n",
       "47    Corrine Gallop          136\n",
       "20       Marthe Hale          128\n",
       "17     Julieta Knapp           25\n",
       "37  Augusta Beltrami           25\n",
       "38    Grover Gibbons           25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find max time employees pauses for all employees\n",
    "\n",
    "employees = complete_df['Author'].unique().tolist()\n",
    "d = []\n",
    "for names in employees:\n",
    "    df_f =  complete_df.loc[complete_df['Author']==names]\n",
    "    diff = df_f['Date'].diff().max()\n",
    "    #print(diff)\n",
    "    d.append([names,diff])\n",
    "    #print(names, df_f['Date'].diff().max())\n",
    "\n",
    "#create a dataframe from the list\n",
    "overview = pd.DataFrame(d,columns=['Author', 'Max_Absence'])\n",
    "#convert to int\n",
    "overview['Max_Absence'] = overview['Max_Absence'].dt.days\n",
    "#sort the values\n",
    "overview = overview.sort_values(by='Max_Absence', ascending=False)\n",
    "\n",
    "#show the head.\n",
    "overview.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37fc9b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Looking at the maximum time between publishing dates their are two clear outliers: Corrine Gallop and Marthe Hale. Both have around 130 days leave, with the 3rd highest time between posting being only 25 days. As mentioned earlier there could be many causes for this time between posting, but we will asume Corrine Gallop and Marthe Hale were on pregnancy leave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ec10a",
   "metadata": {},
   "source": [
    "## 2.4 Holidays (Question c)\n",
    "*If you would be looking to work for Tabularazor Inc., how many holidays can you expect to get per\n",
    "year?*\n",
    "\n",
    "To answer how many holidays a person can expect when working at Tabularazor we will look at the amount of days an author was not publishing, ignoring the weekend days and days not employed. To do this we checked for each employee the ratio of days off over the days the employee is \"supposed\" to work. For each extra day worked in the weekend we substracted a holiday as the employee then worked more than the standard work days.\n",
    "\n",
    "After calculating the ratio's we multiply this by the standard number of yearly working days to find the average amount of yearly holidays for each employee. To calculate the total average yearly holidays we calculated the average of all the employees average yearly holidays.\n",
    "\n",
    "This method was useful for finding the average yearly holidays per employee. However to calculate the average of all employees it's more accurate to look at the total working days of all employees and days off of all employees. This way employees that worked at Tabularazor longer have more influence over the average than a person only working there temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d841425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average yearly holidays: 29\n"
     ]
    }
   ],
   "source": [
    "off_days = 0\n",
    "work_days = 0\n",
    "\n",
    "for employee in employees_df.iterrows():    \n",
    "        \n",
    "    for day in employee[1]: \n",
    "        \n",
    "        #Checking employee days for:\n",
    "        #0 - not_employed\n",
    "        #1 - Working\n",
    "        #2 - Offday\n",
    "        #3 - weekend_Working\n",
    "        #4 - weekend_Offday\n",
    "        \n",
    "        #Count the working days when this employee was employed\n",
    "        if day != 0 and day != 4:\n",
    "            work_days += 1\n",
    "        #Count days off\n",
    "        if day == 2:\n",
    "            off_days += 1\n",
    "        #Remove extra days worked from off days\n",
    "        elif day == 3:\n",
    "            off_days -= 1\n",
    "            \n",
    "ratio = off_days/work_days\n",
    "\n",
    "yearly_work_days = (5 *365/7)\n",
    "\n",
    "average_holidays = round(yearly_work_days * ratio)\n",
    "\n",
    "print(\"Average yearly holidays: \" + str(average_holidays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d72a85",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The average number of holidays an employee can expect is 29. \n",
    "\n",
    "The number does seem to differ a lot over different employees. Roughly speaking people that worked there less (likely temporary writers) get more days off. Though, there are also other factors influencing these extra days off, like extra leave due to pregnancy. With the code below we show the distribution of holidays across all employees. We also calculate the average number of holidays taken over all employees (instead of all days worked like done above). This is less accurate, but does show the difference between employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd8ecf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average yearly holidays over employees: 31\n",
      "Average yearly holidays over days worked: 29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Off days</th>\n",
       "      <th>Work days</th>\n",
       "      <th>Off day ratio</th>\n",
       "      <th>Yearly holidays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adriene Wilde</th>\n",
       "      <td>255</td>\n",
       "      <td>812</td>\n",
       "      <td>0.314039</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andreas Jost</th>\n",
       "      <td>110</td>\n",
       "      <td>2140</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angela Compos</th>\n",
       "      <td>152</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.151848</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anisha Mathes</th>\n",
       "      <td>332</td>\n",
       "      <td>2135</td>\n",
       "      <td>0.155504</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annie Lapham</th>\n",
       "      <td>506</td>\n",
       "      <td>2132</td>\n",
       "      <td>0.237336</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Augusta Beltrami</th>\n",
       "      <td>63</td>\n",
       "      <td>808</td>\n",
       "      <td>0.077970</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aurea Durrance</th>\n",
       "      <td>336</td>\n",
       "      <td>2138</td>\n",
       "      <td>0.157156</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bebe Riva</th>\n",
       "      <td>114</td>\n",
       "      <td>2145</td>\n",
       "      <td>0.053147</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bridgette Marko</th>\n",
       "      <td>117</td>\n",
       "      <td>2139</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Byron Caroll</th>\n",
       "      <td>47</td>\n",
       "      <td>797</td>\n",
       "      <td>0.058971</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chung Zertuche</th>\n",
       "      <td>407</td>\n",
       "      <td>795</td>\n",
       "      <td>0.511950</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corrine Gallop</th>\n",
       "      <td>251</td>\n",
       "      <td>997</td>\n",
       "      <td>0.251755</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donte Berkeley</th>\n",
       "      <td>74</td>\n",
       "      <td>1336</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grover Gibbons</th>\n",
       "      <td>69</td>\n",
       "      <td>802</td>\n",
       "      <td>0.086035</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harmony Mcmurtry</th>\n",
       "      <td>130</td>\n",
       "      <td>2137</td>\n",
       "      <td>0.060833</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hassan Toner</th>\n",
       "      <td>694</td>\n",
       "      <td>2084</td>\n",
       "      <td>0.333013</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hettie Beller</th>\n",
       "      <td>59</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hwa Defalco</th>\n",
       "      <td>110</td>\n",
       "      <td>2146</td>\n",
       "      <td>0.051258</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jarvis Pratts</th>\n",
       "      <td>303</td>\n",
       "      <td>2142</td>\n",
       "      <td>0.141457</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaye Shimek</th>\n",
       "      <td>126</td>\n",
       "      <td>2128</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jeannie Liang</th>\n",
       "      <td>114</td>\n",
       "      <td>2139</td>\n",
       "      <td>0.053296</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Johnetta Hatcher</th>\n",
       "      <td>32</td>\n",
       "      <td>805</td>\n",
       "      <td>0.039752</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Julieta Knapp</th>\n",
       "      <td>139</td>\n",
       "      <td>2127</td>\n",
       "      <td>0.065350</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiera Loar</th>\n",
       "      <td>361</td>\n",
       "      <td>1002</td>\n",
       "      <td>0.360279</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kyle Ryals</th>\n",
       "      <td>105</td>\n",
       "      <td>2089</td>\n",
       "      <td>0.050263</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lavonna Sim</th>\n",
       "      <td>122</td>\n",
       "      <td>2136</td>\n",
       "      <td>0.057116</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layne Woodside</th>\n",
       "      <td>115</td>\n",
       "      <td>2144</td>\n",
       "      <td>0.053638</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leandro Marse</th>\n",
       "      <td>69</td>\n",
       "      <td>1339</td>\n",
       "      <td>0.051531</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leatrice Heer</th>\n",
       "      <td>130</td>\n",
       "      <td>2131</td>\n",
       "      <td>0.061004</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lester Preiss</th>\n",
       "      <td>130</td>\n",
       "      <td>2133</td>\n",
       "      <td>0.060947</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lisette Tolleson</th>\n",
       "      <td>98</td>\n",
       "      <td>2143</td>\n",
       "      <td>0.045730</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lorrine Paek</th>\n",
       "      <td>715</td>\n",
       "      <td>2142</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marthe Hale</th>\n",
       "      <td>148</td>\n",
       "      <td>533</td>\n",
       "      <td>0.277674</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mellisa Blanco</th>\n",
       "      <td>154</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.144195</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michale Tuch</th>\n",
       "      <td>133</td>\n",
       "      <td>2129</td>\n",
       "      <td>0.062471</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Milagros Salmeron</th>\n",
       "      <td>110</td>\n",
       "      <td>2151</td>\n",
       "      <td>0.051139</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narcisa Core</th>\n",
       "      <td>127</td>\n",
       "      <td>2133</td>\n",
       "      <td>0.059541</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paulene Calcagni</th>\n",
       "      <td>102</td>\n",
       "      <td>2148</td>\n",
       "      <td>0.047486</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Petrina Maggart</th>\n",
       "      <td>44</td>\n",
       "      <td>799</td>\n",
       "      <td>0.055069</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Randall Feather</th>\n",
       "      <td>272</td>\n",
       "      <td>2134</td>\n",
       "      <td>0.127460</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Remedios Fincham</th>\n",
       "      <td>318</td>\n",
       "      <td>2131</td>\n",
       "      <td>0.149226</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sal Papa</th>\n",
       "      <td>112</td>\n",
       "      <td>2146</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stephine Gatlin</th>\n",
       "      <td>127</td>\n",
       "      <td>2137</td>\n",
       "      <td>0.059429</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tasia Dube</th>\n",
       "      <td>313</td>\n",
       "      <td>2134</td>\n",
       "      <td>0.146673</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tora Echevarria</th>\n",
       "      <td>670</td>\n",
       "      <td>2144</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vance Ates</th>\n",
       "      <td>51</td>\n",
       "      <td>798</td>\n",
       "      <td>0.063910</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Velvet Matley</th>\n",
       "      <td>81</td>\n",
       "      <td>1565</td>\n",
       "      <td>0.051757</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Veronique Blakemore</th>\n",
       "      <td>113</td>\n",
       "      <td>2145</td>\n",
       "      <td>0.052681</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vonk Billips</th>\n",
       "      <td>126</td>\n",
       "      <td>2143</td>\n",
       "      <td>0.058796</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zofia Roll</th>\n",
       "      <td>119</td>\n",
       "      <td>2136</td>\n",
       "      <td>0.055712</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Off days  Work days  Off day ratio  Yearly holidays\n",
       "Author                                                                  \n",
       "Adriene Wilde             255        812       0.314039               82\n",
       "Andreas Jost              110       2140       0.051402               13\n",
       "Angela Compos             152       1001       0.151848               40\n",
       "Anisha Mathes             332       2135       0.155504               41\n",
       "Annie Lapham              506       2132       0.237336               62\n",
       "Augusta Beltrami           63        808       0.077970               20\n",
       "Aurea Durrance            336       2138       0.157156               41\n",
       "Bebe Riva                 114       2145       0.053147               14\n",
       "Bridgette Marko           117       2139       0.054698               14\n",
       "Byron Caroll               47        797       0.058971               15\n",
       "Chung Zertuche            407        795       0.511950              133\n",
       "Corrine Gallop            251        997       0.251755               66\n",
       "Donte Berkeley             74       1336       0.055389               14\n",
       "Grover Gibbons             69        802       0.086035               22\n",
       "Harmony Mcmurtry          130       2137       0.060833               16\n",
       "Hassan Toner              694       2084       0.333013               87\n",
       "Hettie Beller              59       1001       0.058941               15\n",
       "Hwa Defalco               110       2146       0.051258               13\n",
       "Jarvis Pratts             303       2142       0.141457               37\n",
       "Jaye Shimek               126       2128       0.059211               15\n",
       "Jeannie Liang             114       2139       0.053296               14\n",
       "Johnetta Hatcher           32        805       0.039752               10\n",
       "Julieta Knapp             139       2127       0.065350               17\n",
       "Kiera Loar                361       1002       0.360279               94\n",
       "Kyle Ryals                105       2089       0.050263               13\n",
       "Lavonna Sim               122       2136       0.057116               15\n",
       "Layne Woodside            115       2144       0.053638               14\n",
       "Leandro Marse              69       1339       0.051531               13\n",
       "Leatrice Heer             130       2131       0.061004               16\n",
       "Lester Preiss             130       2133       0.060947               16\n",
       "Lisette Tolleson           98       2143       0.045730               12\n",
       "Lorrine Paek              715       2142       0.333800               87\n",
       "Marthe Hale               148        533       0.277674               72\n",
       "Mellisa Blanco            154       1068       0.144195               38\n",
       "Michale Tuch              133       2129       0.062471               16\n",
       "Milagros Salmeron         110       2151       0.051139               13\n",
       "Narcisa Core              127       2133       0.059541               16\n",
       "Paulene Calcagni          102       2148       0.047486               12\n",
       "Petrina Maggart            44        799       0.055069               14\n",
       "Randall Feather           272       2134       0.127460               33\n",
       "Remedios Fincham          318       2131       0.149226               39\n",
       "Sal Papa                  112       2146       0.052190               14\n",
       "Stephine Gatlin           127       2137       0.059429               15\n",
       "Tasia Dube                313       2134       0.146673               38\n",
       "Tora Echevarria           670       2144       0.312500               81\n",
       "Vance Ates                 51        798       0.063910               17\n",
       "Velvet Matley              81       1565       0.051757               13\n",
       "Veronique Blakemore       113       2145       0.052681               14\n",
       "Vonk Billips              126       2143       0.058796               15\n",
       "Zofia Roll                119       2136       0.055712               15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_days = []\n",
    "work_days = []\n",
    "ratios = []\n",
    "yearly_holidays = []\n",
    "\n",
    "#Amount of non weekend days per year\n",
    "yearly_work_days = (5 *365/7)\n",
    "\n",
    "for employee in employees_df.iterrows():    \n",
    "    \n",
    "    off_days_count = 0\n",
    "    work_days_count = 0\n",
    "    \n",
    "    for day in employee[1]: \n",
    "        \n",
    "        #Checking employee days for:\n",
    "        #0 - Not_employed\n",
    "        #1 - Working\n",
    "        #2 - Offday\n",
    "        #3 - Weekend_Working\n",
    "        #4 - Weekend_Offday\n",
    "        \n",
    "        #Count the working days when this employee was employed\n",
    "        if day != 0 and day != 4:\n",
    "            work_days_count += 1\n",
    "        #Count days off\n",
    "        if day == 2:\n",
    "            off_days_count += 1\n",
    "        #Remove extra days worked from off days\n",
    "        elif day == 3:\n",
    "            off_days_count-= 1\n",
    "    \n",
    "    off_days.append(off_days_count)\n",
    "    work_days.append(work_days_count)\n",
    "    \n",
    "    ratio = off_days_count/work_days_count\n",
    "    ratios.append(ratio)\n",
    "    \n",
    "    yearly_holidays.append(round(ratio * yearly_work_days))\n",
    " \n",
    "employees_df[\"Off days\"] = off_days\n",
    "employees_df[\"Work days\"] = work_days\n",
    "employees_df[\"Off day ratio\"] = ratios\n",
    "employees_df[\"Yearly holidays\"] = yearly_holidays\n",
    "\n",
    "average_holidays_employee = round(employees_df[\"Yearly holidays\"].mean())\n",
    "\n",
    "print(\"Average yearly holidays over employees: \" + str(average_holidays_employee))\n",
    "print(\"Average yearly holidays over days worked: \" + str(average_holidays))\n",
    "\n",
    "display_df = employees_df[[\"Off days\", \"Work days\", \"Off day ratio\", \"Yearly holidays\"]]\n",
    "\n",
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20169b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
