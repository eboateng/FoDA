{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEN163A - Fundamentals of Data Analytics\n",
    "# Assignment 2 - Large-scale Internet Data Analysis\n",
    "### Ir. Jacopo De Stefani - [J.deStefani@tudelft.nl](mailto:J.deStefani@tudelft.nl)\n",
    "### Joao Pizani Flor, M.Sc. - [J.p.pizaniflor@tudelft.nl](mailto:J.p.pizaniflor@tudelft.nl)\n",
    "\n",
    "### 05-03-2022\n",
    "## Group 2\n",
    "- Emmanuel M Boateng - '5617642'\n",
    "- Joost Oortwijn - '4593472'\n",
    "- Philip Busscher - ''4611993''\n",
    "- Floris Kool - ''4975243''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This report contains a data-analysis for The Groote Nationale Investeer Bank to investigate the four most convenient locations for a datacenter in the EU. Four datasets are provided to carry out the research. Using Jupyter Notebook, the datasets are structured and prepared for analysis. \n",
    "The provided data sets are a RIPE data set, an IP location data set, an AS (Automated System) data set and a probe data set. Next paragraph gives a brieve explanation on the data sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset description\n",
    "\n",
    "The AS data set contains ASN (Autonomous System Numbers) that represent different a groups of IP adresses run by a network operator. The country in which the network is situated, number of IP's and types are also displayed in this dataframe. This data set is important to locate where the probes are situated. \n",
    "\n",
    "The probe data set contains two attributes: the id of the probe and an ASN number. Using this data set one can assign a probe to a specific ASN in a specific country. \n",
    "\n",
    "The RIPE data set contains ping measurements of approximately 30 days. A day consists of twentyfour files which contains ping meauserments for one hour. A ping measurement is the time that a small package of data travels between your device, a server and back to your device. In this research only the ping measurements of the 1th of March is used. \n",
    "\n",
    "The last data set is the IP location of the senders and the receivers of the data packages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Opening the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASN</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "      <th>NumIPs</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS55330</td>\n",
       "      <td>AF</td>\n",
       "      <td>AFGHANTELECOM GOVERNMENT COMMUNICATION NETWORK</td>\n",
       "      <td>50,432</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS17411</td>\n",
       "      <td>AF</td>\n",
       "      <td>Io Global Services Pvt. Limited</td>\n",
       "      <td>13,568</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AS55424</td>\n",
       "      <td>AF</td>\n",
       "      <td>Instatelecom Limited</td>\n",
       "      <td>13,312</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AS38742</td>\n",
       "      <td>AF</td>\n",
       "      <td>AWCC</td>\n",
       "      <td>11,520</td>\n",
       "      <td>isp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AS131284</td>\n",
       "      <td>AF</td>\n",
       "      <td>Etisalat Afghan</td>\n",
       "      <td>10,240</td>\n",
       "      <td>isp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AS45178</td>\n",
       "      <td>AF</td>\n",
       "      <td>ROSHAN-AF</td>\n",
       "      <td>5,376</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AS132471</td>\n",
       "      <td>AF</td>\n",
       "      <td>MTNAFGHANISTAN</td>\n",
       "      <td>4,864</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AS7494</td>\n",
       "      <td>AF</td>\n",
       "      <td>CeReTechs Co ltd</td>\n",
       "      <td>4,096</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AS138322</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghan Wireless</td>\n",
       "      <td>3,584</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AS55745</td>\n",
       "      <td>AF</td>\n",
       "      <td>Neda Telecommunications</td>\n",
       "      <td>2,560</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ASN Country                                            Name  NumIPs  \\\n",
       "0   AS55330      AF  AFGHANTELECOM GOVERNMENT COMMUNICATION NETWORK  50,432   \n",
       "1   AS17411      AF                 Io Global Services Pvt. Limited  13,568   \n",
       "2   AS55424      AF                            Instatelecom Limited  13,312   \n",
       "3   AS38742      AF                                            AWCC  11,520   \n",
       "4  AS131284      AF                                 Etisalat Afghan  10,240   \n",
       "5   AS45178      AF                                       ROSHAN-AF   5,376   \n",
       "6  AS132471      AF                                  MTNAFGHANISTAN   4,864   \n",
       "7    AS7494      AF                                CeReTechs Co ltd   4,096   \n",
       "8  AS138322      AF                                 Afghan Wireless   3,584   \n",
       "9   AS55745      AF                         Neda Telecommunications   2,560   \n",
       "\n",
       "       type  \n",
       "0   hosting  \n",
       "1  business  \n",
       "2  business  \n",
       "3       isp  \n",
       "4       isp  \n",
       "5  business  \n",
       "6  business  \n",
       "7  business  \n",
       "8  business  \n",
       "9  business  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AS Dataset\n",
    "\n",
    "AS_Filename = 'data\\AS_dataset.pkl'\n",
    "\n",
    "with open(AS_Filename, 'rb') as file:\n",
    "    \n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "AS_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prb_id</th>\n",
       "      <th>ASN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AS3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AS1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AS3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>AS6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>AS3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>AS12333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>AS3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>AS3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>AS7922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26</td>\n",
       "      <td>AS3265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prb_id      ASN\n",
       "0      1   AS3265\n",
       "1      2   AS1136\n",
       "2      3   AS3265\n",
       "3      6   AS6830\n",
       "4      8   AS3265\n",
       "5     11  AS12333\n",
       "6     14   AS3269\n",
       "7     20   AS3265\n",
       "8     24   AS7922\n",
       "9     26   AS3265"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Probe dataset\n",
    "\n",
    "Probe_Filename = 'data/probe_dataset.pkl'\n",
    "\n",
    "with open(Probe_Filename, 'rb') as file:\n",
    "    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "P_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decomFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ef27dcae4a61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#for line in bz2File:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdecomFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdecoded_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decomFile' is not defined"
     ]
    }
   ],
   "source": [
    "#Ripe dataset (Single file)\n",
    "\n",
    "#Option 1 decompressed file\n",
    "#decomFilename = 'data/ping-2022-03-01T2300_decom'\n",
    "#decomFile     = open(decomFilename, 'rt')\n",
    "\n",
    "#Option 2 BZ2 file\n",
    "bz2Filename = 'data/ping-2022-03-01T2300.bz2'\n",
    "bz2File     = bz2.open(bz2Filename, 'rt')\n",
    "\n",
    "\n",
    "# List of tuples\n",
    "# https://stackoverflow.com/questions/28056171/how-to-build-and-fill-pandas-dataframe-from-for-loop\n",
    "tuple_list = []\n",
    "\n",
    "start  = time.time()\n",
    "\n",
    "#for line in bz2File:\n",
    "for line in decomFile:\n",
    "    \n",
    "    decoded_line = json.loads(line)\n",
    "    if \"af\" in decoded_line and \"dst_addr\" in decoded_line and \"prb_id\" in decoded_line and \"avg\" in decoded_line: \n",
    "        if decoded_line[\"af\"] == 4:\n",
    "            tuple_list.append((decoded_line[\"dst_addr\"],decoded_line[\"prb_id\"],decoded_line[\"avg\"]))\n",
    "\n",
    "            \n",
    "dur         = round(time.time() - start,2)\n",
    "print(\"Loading took: \"  + str(dur) + \" seconds\")\n",
    "print(\"Lines added to tuple: \" + str(len(tuple_list)))\n",
    "\n",
    "#finally close bz2File\n",
    "decomFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tuples data into dataframe\n",
    "start  = time.time()\n",
    "\n",
    "RIPE_df = pd.DataFrame(tuple_list)\n",
    "\n",
    "dur         = round(time.time() - start,2)\n",
    "print(\"Loading took: \"  + str(dur) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP 2 Location dataset\n",
    "\n",
    "IP_Filename = \"data/IP2LOCATION-LITE-DB1.CSV\"\n",
    "\n",
    "ipv4_df = pd.read_csv(IP_Filename)\n",
    "\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "ipv4_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed description of data if needed (Can also be after opening each dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Limitations in data\n",
    "\n",
    "Evaluate if there are limitations in the provided datasets (AS and probe data set). If you find limitations, describe these and conjecture possible reasons, supported with data.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code needed to prove limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some list of limitations in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Analysis\n",
    "\n",
    "Short description of what is going to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 AS (Question B)\n",
    "\n",
    "With the AS and probe data set, find the number m of AS’s that can be used for hosting in the EU\n",
    "and have probes in the RIPE data set. Sort the ASN’s in ascending order and include the first and last\n",
    "three in your report (number, name and country).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the AS and Probe datasets\n",
    "as_probe_joined_df = pd.merge(P_df,AS_df, on='ASN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EU country codes retrieved from: https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Country_codes\n",
    "EU_list = ['BE','BG','CZ','DK','DE','EE','IE','EL','ES','FR','HR','IT','CY','LV','LT','LU','HU','MT','NL','AT','PL','PT','RO','SI','SK','FI','SE']\n",
    "\n",
    "# Filter data set for AS's that can be used for hosting in the EU\n",
    "as_probe_joined_df = as_probe_joined_df.loc[(as_probe_joined_df['type'] == 'hosting') & (as_probe_joined_df['Country'].isin(EU_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the unique number of probe IDs that are in the RIPE Data\n",
    "unique_prbID = RIPE_df[1].unique()\n",
    "\n",
    "print(\"Unique probe IDs: \" + str(len(unique_prbID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data set by only selecting the ASN's that have probes in the Ripe dataset\n",
    "AS_Probe_RIPE_df = as_probe_joined_df.loc[as_probe_joined_df['prb_id'].isin(unique_prbID)]\n",
    "\n",
    "#Sort by ASN\n",
    "AS_Probe_RIPE_df.sort_values(by=['ASN']).sort_values(by=['ASN'])\n",
    "\n",
    "print(\"Number of probes connected to AS that can be used for hosting in the EU and are in the RIPE dataset: \" + str(len(AS_Probe_RIPE_df[\"ASN\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate ASNs (Probes connected to same AS)\n",
    "display_df = AS_Probe_RIPE_df.drop_duplicates(subset=['ASN'])\n",
    "\n",
    "#Remove unused columns\n",
    "display_df = display_df.drop(columns=['prb_id', 'NumIPs', 'type'])\n",
    "\n",
    "#Print anwser to question B\n",
    "print(\"Number of AS that can be used for hosting in the EU and are in the RIPE dataset: \" + str(len(display_df[\"ASN\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 3 probes\n",
    "display_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last 3 probes\n",
    "display_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hosting location (Question C)\n",
    "For a single hour in the RIPE data set: find all valid entries where the probe has hosting type AS and\n",
    "the target IPv4 is from an EU country. Implement this in an efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selects all entries in RIPE data with probe connected to EU as of type hosting\n",
    "RIPE_HostAS_df = RIPE_df.loc[RIPE_df[1].isin(AS_Probe_RIPE_df['prb_id'])]\n",
    "\n",
    "print(\"Entries with probe connected to an EU as with type hosting: \" + str(len(RIPE_HostAS_df[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert IP strings to IP integers\n",
    "for i in RIPE_HostAS_df.index:\n",
    "    \n",
    "    if i == 0:\n",
    "        print(RIPE_HostAS_df[0][i])\n",
    "    IP_Splitstring = RIPE_HostAS_df[0][i].split(\".\") \n",
    "    RIPE_HostAS_df.at[i, 0] = int(IP_Splitstring[0]) * 16581375 + int(IP_Splitstring[1]) * 65025 + int(IP_Splitstring[2]) * 255 + int(IP_Splitstring[3])\n",
    "\n",
    "\n",
    "\n",
    "#Add Integer values of IP to dataframe\n",
    "#RIPE_HostAS_df[\"IP_Integer\"] = IPs_Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the IP lists so we can check from low to high IPs\n",
    "RIPE_HostAS_df = RIPE_HostAS_df.sort_values(by=[0])\n",
    "ipv4_df = ipv4_df.sort_values(by=[\"ip_from\"])\n",
    "\n",
    "Dest_Addr_Countries = []\n",
    "ripeindex = 0\n",
    "ipindex = 0\n",
    "\n",
    "#Check if there are IP addresses lower than included in the IP2Location dataset\n",
    "while RIPE_HostAS_df.iat[ripeindex, 0] < ipv4_df.at[ipindex, \"ip_from\"]:\n",
    "    ripeindex = ripeindex + 1\n",
    "    Dest_Addr_Countries.append(\"-\")\n",
    "\n",
    "print(\"IP addresses not included in IP2location dataset: \" + str(ripeindex))\n",
    "\n",
    "#Check for each range of IP addresses in the IP2Location dataset which dst_addr IPs are present\n",
    "#Break loop early if the length of the RIPE dataset is reached\n",
    "for ipindex in ipv4_df.index:\n",
    "    while RIPE_HostAS_df.iat[ripeindex, 0] >= ipv4_df.at[ipindex, \"ip_from\"] and RIPE_HostAS_df.iat[ripeindex, 0] <= ipv4_df.at[ipindex, \"ip_to\"]:\n",
    "        Dest_Addr_Countries.append(ipv4_df.at[ipindex, \"country_code\"])\n",
    "        ripeindex = ripeindex + 1\n",
    "        if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "            break\n",
    "    \n",
    "    if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "        break\n",
    "\n",
    "print(\"IP addresses linked to country: \" + str(len(Dest_Addr_Countries)))\n",
    "#Add list for destination address location to dataframe\n",
    "RIPE_HostAS_df[\"Country\"] = Dest_Addr_Countries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove entries not in EU\n",
    "RIPE_HostAS_df = RIPE_HostAS_df.loc[RIPE_HostAS_df['Country'].isin(EU_list)]\n",
    "\n",
    "print(\"Entries with probe connected to an EU AS with type hosting and destination address within EU: \" + str(len(RIPE_HostAS_df[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Alternative Approach reading all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "#decomFile     = bz2.open(decomFilename, 'rt')   \n",
    "merged_df = P_df.merge(AS_df)\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]\n",
    "EU_Hosting = EU_data[EU_data['type'] == 'hosting']\n",
    "\n",
    "\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "\n",
    "merged_df_sorted = merged_df.sort_values('AS')\n",
    "df_HostingAS = merged_df[merged_df['type'] == 'hosting']\n",
    "\n",
    "ipv4_df.head()\n",
    "tpl = ipv4_df.loc[:, 'ip_from':'ip_to'].apply(tuple, 1).tolist()\n",
    "idx = pandas.IntervalIndex.from_tuples(tpl, 'both')\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(0.000001)\n",
    "with open(decomFilename, 'rb') as file:\n",
    "    decomp = bz2.BZ2Decompressor()\n",
    "    residue = b''\n",
    "    total_lines = 0\n",
    "    m = 0\n",
    "    checked = []\n",
    "    #102400 Bytes = 102.4 KB (in decimal)\n",
    "    #102400 Bytes = 100 KB (in binary)\n",
    "    #Iterate over RIPE data in  100 KB chunks \n",
    "    for data in iter(lambda: file.read(100 * 1024), b''):\n",
    "        # process the raw data and  concatenate residual of the previous block \n",
    "        #to the beginning of the current raw data block\n",
    "        raw = residue + decomp.decompress(data) \n",
    "        residue = b''\n",
    "        ## process_data(current_block) => do the processing of the \n",
    "        ##current data block\n",
    "        current_block = raw.split(b'\\n')\n",
    "        if raw[-1] != b'\\n':\n",
    "            residue = current_block.pop() # last line could be incomplete\n",
    "        ##Process all data in the current block to check    \n",
    "        for items in current_block:\n",
    "            df_dict = json.loads(items.decode('utf-8'))\n",
    "            if ('dst_addr' in df_dict) and (df_dict['af'] == 4):# and (ip2int(df_dict['dst_addr'])>0:\n",
    "                ##convert to interger\n",
    "                df_ip = ip2int(df_dict['dst_addr'])\n",
    "                #print(df_ip)\n",
    "                if df_ip > 0: # certain lines have 0.0.0.0 IP\n",
    "                    loc = idx.get_loc(df_ip)\n",
    "                    if ((ipv4_df.loc[loc,'country_code'] in EU_Countries) and (df_dict['prb_id'] not in checked)):\n",
    "                        #if len(EU_Hosting[EU_Hosting['prb_id'] == df_dict['prb_id']])!=0:\n",
    "                            #print(df_HostingAS[df_HostingAS['prb_id'] == df_dict['prb_id']])\n",
    "                        m +=1 ## increment count\n",
    "                       ##create a list of probes that could be used later                     \n",
    "                        checked.append(df_dict['prb_id']) \n",
    "        total_lines += len(current_block)\n",
    "    total_lines += 1\n",
    "\n",
    "print(\"Total processing time: \",(time.time() - t0))\n",
    "print(\"Total number of probe entries with hosting type AS and EU target in RIPE is %i\" %(m))\n",
    "fi.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Latency (Question D)\n",
    "Move from using only an hour to the full day. It is advisable to store the raw results of each file. Then,\n",
    "using all processed files, calculate the average latency’s for each country-AS combination and store\n",
    "the results into one ncountries ×m matrix. If we could place one server in each country, what would the\n",
    "minimum average latency be for each country? (include in your report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want a matrix of 26 countries * 113 ASNs (For a single file, should be more for 24 files)\n",
    "\n",
    "#tuple list\n",
    "test1= []\n",
    "\n",
    "for country in EU_list:\n",
    "    \n",
    "    #Filter each country's ping values seperately into a dataframe\n",
    "    country_df = RIPE_HostAS_df.loc[RIPE_HostAS_df['Country'] == country]\n",
    "    \n",
    "    for ASN in display_df[\"ASN\"]:\n",
    "        \n",
    "        #Filter probe IDs for each seperate ASN\n",
    "        #There are more probes than ASs to calculate the average ping more accurately we use all probes\n",
    "        prb_df = AS_Probe_RIPE_df.loc[AS_Probe_RIPE_df['ASN'] == ASN]                            \n",
    "        \n",
    "        #Filter the ping data so it includes all probes from selected ASN and selected country\n",
    "        temp_df = country_df.loc[country_df[1].isin(prb_df['prb_id'])]\n",
    "        \n",
    "        #Create sum of all ASN - Country ping measurements\n",
    "        sumvalue = 0\n",
    "        i = 0\n",
    "        for pingvalue in temp_df[2]:\n",
    "            sumvalue = sumvalue + pingvalue\n",
    "            i = i+1\n",
    "        \n",
    "        #Check if there are ping measurements between AS - Country\n",
    "        #Calculate average when needed, enter '-' when no data available\n",
    "        if not i == 0:\n",
    "            average = sumvalue/i\n",
    "            test1.append((country, ASN, average))\n",
    "        else:\n",
    "            test1.append((country, ASN, '-'))\n",
    "\n",
    "#Load tuple list into dataframe\n",
    "test1_df = pd.DataFrame(test1)        \n",
    "test1_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Optimal server locations (Question E)\n",
    "Since we are only allowed to place four servers, determine the best four datacenters based on the total\n",
    "latency for all countries. Report your findings and your procedure to obtain them. Also include the\n",
    "average latency for each country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "... \n",
    "add code if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbes = []\n",
    "\n",
    "for ProbeASN in HostProbeASNs:\n",
    "    \n",
    "    index = 0\n",
    "    for ASN in P_df[\"ASN\"]:\n",
    "        if ASN == ProbeASN:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "            break\n",
    "        index = index + 1\n",
    "        \n",
    "print(len(HostProbes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "\n",
    "HostIPs = []\n",
    "index = 0\n",
    "\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    \n",
    "    if jsonline[\"prb_id\"] in HostProbes:\n",
    "        try:\n",
    "            #Check for duplicates\n",
    "            if jsonline[\"dst_addr\"] not in HostIPs:\n",
    "                #Check if IP is of type 4\n",
    "                if jsonline[\"af\"] == 4:\n",
    "                    HostIPs.append(jsonline[\"dst_addr\"])\n",
    "        except KeyError as err:\n",
    "            pass\n",
    "    \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "                              \n",
    "print(\"Amount of IPs in the RIPE data connected to an AS of type Hosting: \" + str(len(HostIPs)))\n",
    "\n",
    "decomFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the xxx.xxx.xxx.xxx format of the host IPs to integer\n",
    "#Needed for when comparing to the IPv4 dataset\n",
    "\n",
    "HostIPs_Integer = []\n",
    "\n",
    "\n",
    "for IPString in HostIPs:\n",
    "    IP_Splitstring = IPString.split(\".\") \n",
    " \n",
    "    HostIPs_Integer.append(int(IP_Splitstring[0]) * 16581375 + int(IP_Splitstring[1]) * 65025 + int(IP_Splitstring[2]) * 255 + int(IP_Splitstring[3]))  \n",
    "\n",
    "print(len(HostIPs_Integer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upper part should be removed because run in part 1\n",
    "import pickle\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "\n",
    "RIPEProbes = []\n",
    "index = 0\n",
    "\n",
    "#Create list of all probes that are in the RIPE dataset\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    if jsonline[\"prb_id\"] not in RIPEProbes:\n",
    "        RIPEProbes.append(jsonline[\"prb_id\"])\n",
    "                          \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "        \n",
    "                  \n",
    "print(\"Probes in first 1m lines of RIPE Dataset: \" +str(len(RIPEProbes)))            \n",
    "\n",
    "decomFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of country codes that are an EU member\n",
    "\n",
    "EU_Countries = [\"AT\",\n",
    "    \"BE\",\n",
    "    \"HR\",\n",
    "    \"CY\",\n",
    "    \"CZ\",\n",
    "    \"DK\",\n",
    "    \"EE\",\n",
    "    \"FI\",\n",
    "    \"FR\",\n",
    "    \"GR\",\n",
    "    \"DE\",\n",
    "    \"HU\",\n",
    "    \"IE\",\n",
    "    \"IT\",\n",
    "    \"LV\",\n",
    "    \"LT\",\n",
    "    \"LU\",\n",
    "    \"MT\",\n",
    "    \"NL\",\n",
    "    \"PL\",\n",
    "    \"PT\",\n",
    "    \"RO\",\n",
    "    \"SK\",\n",
    "    \"SI\",\n",
    "    \"ES\",\n",
    "    \"SE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbeASNs = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for ASN in AS_df[\"ASN\"]:\n",
    "    \n",
    "    if ASN in ProbeASNs:\n",
    "        if AS_df[\"type\"][index] == \"hosting\":\n",
    "            HostProbeASNs.append(ASN)            \n",
    "    index = index + 1    \n",
    "    \n",
    "print(\"Amount of probes with an ASN with type hosting: \" + str(len(HostProbeASNs))) \n",
    "\n",
    "HostProbeASNs.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare IPv4 with HostIPs_Integer\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "ipv4_df.head()\n",
    "\n",
    "HostIPs_EU = []\n",
    "index = 0\n",
    "ipv4index = 0\n",
    "\n",
    "#Sorting the IP list so we can check from low to high IPs\n",
    "HostIPs_Integer.sort\n",
    "\n",
    "for IP_to in ipv4_df[\"ip_to\"]:\n",
    "    \n",
    "    while HostIPs_Integer[index] < IP_to:\n",
    "        if ipv4_df[\"country_code\"][ipv4index] in EU_Countries:\n",
    "            HostIPs_EU.append(HostIPs_Integer[index])\n",
    "            print(HostIPs_Integer[index])\n",
    "            print(ipv4_df[\"country_code\"][ipv4index])\n",
    "        index = index + 1\n",
    "        if index >= len(HostIPs_Integer):\n",
    "            break;\n",
    "    ipv4index = ipv4index + 1  \n",
    "    if index >= len(HostIPs_Integer):\n",
    "        break;\n",
    "                  \n",
    "\n",
    "print(len(HostIPs_EU))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of unique probe ID's in RIPE dataset\n",
    "unique_prbID = []\n",
    "for i in tuple_list:\n",
    "    if i[1] not in unique_prbID:\n",
    "        unique_prbID.append(i[1])\n",
    "        \n",
    "print(\"Unique probe IDs in RIPE dataset: \" + str(len(unique_prbID.l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbes = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(len(HostProbeASNs))\n",
    "\n",
    "for ASN in P_df[\"ASN\"]:\n",
    "    if ASN in HostProbeASNs:\n",
    "        if P_df[\"prb_id\"][index] not in HostProbes:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "    index = index + 1\n",
    "\n",
    "print(len(HostProbes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0#Random stuff I didn't want to throw away yet\n",
    "#0Code for finding all host probes from EU in the dataset of one hour\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# open decompressed file\n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt') \n",
    "\n",
    "#read first line and print\n",
    "#firstLine = decomFile.readline();\n",
    "#print(firstLine)\n",
    "\n",
    "#the line appears to be json-formatted: pretty print json\n",
    "#firstLineJson = json.loads(firstLine)\n",
    "\n",
    "#read all lines of first file\n",
    "count = 0\n",
    "st    = time.time()\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    #print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "    count = count + 1\n",
    "    if count > 10000: \n",
    "          break\n",
    "\n",
    "#print the last line\n",
    "print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "\n",
    "#print the read duration\n",
    "dur         = round(time.time() - st,2)\n",
    "print(\"Loading took: \" + str(dur) + \" seconds\")\n",
    "print(\"The file had \" + str(count) + \"lines\")\n",
    "\n",
    "#finally close decomFile\n",
    "decomFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
