{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEN163A - Fundamentals of Data Analytics\n",
    "# Assignment 2 - Large-scale Internet Data Analysis\n",
    "### Ir. Jacopo De Stefani - [J.deStefani@tudelft.nl](mailto:J.deStefani@tudelft.nl)\n",
    "### Joao Pizani Flor, M.Sc. - [J.p.pizaniflor@tudelft.nl](mailto:J.p.pizaniflor@tudelft.nl)\n",
    "\n",
    "### 05-03-2022\n",
    "## Group 2\n",
    "- Emmanuel M Boateng - '5617642'\n",
    "- Joost Oortwijn - '4593472'\n",
    "- Philip Busscher - ''4611993''\n",
    "- Floris Kool - ''4975243''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "short description of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset description\n",
    "\n",
    "Short description of the 4 datasets used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Opening the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AS Dataset\n",
    "\n",
    "AS_Filename = 'data/AS_dataset.pkl'\n",
    "\n",
    "with open(AS_Filename, 'rb') as file:\n",
    "    \n",
    "    AS_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probe dataset\n",
    "\n",
    "Probe_Filename = 'data/probe_dataset.pkl'\n",
    "\n",
    "with open(Probe_Filename, 'rb') as file:\n",
    "    \n",
    "    P_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ripe dataset \n",
    "\n",
    "#Method needs to be decided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP 2 Location dataset\n",
    "\n",
    "IP_Filename = \"data/IP2LOCATION-LITE-DB1.CSV\"\n",
    "\n",
    "ipv4_df = pandas.read_csv(IP_Filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed description of data if needed (Can also be after opening each dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Limitations in data\n",
    "\n",
    "Evaluate if there are limitations in the provided datasets (AS and probe data set). If you find limitations, describe these and conjecture possible reasons, supported with data.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code needed to prove limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some list of limitations in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Analysis\n",
    "\n",
    "Short description of what is going to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 AS (Question B)\n",
    "\n",
    "With the AS and probe data set, find the number m of AS’s that can be used for hosting in the EU\n",
    "and have probes in the RIPE data set. Sort the ASN’s in ascending order and include the first and last\n",
    "three in your report (number, name and country).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probes in first 1m lines of RIPE Dataset: 11639\n"
     ]
    }
   ],
   "source": [
    "#Upper part should be removed because run in part 1\n",
    "import pickle\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "\n",
    "RIPEProbes = []\n",
    "index = 0\n",
    "\n",
    "#Create list of all probes that are in the RIPE dataset\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    if jsonline[\"prb_id\"] not in RIPEProbes:\n",
    "        RIPEProbes.append(jsonline[\"prb_id\"])\n",
    "                          \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "        \n",
    "                  \n",
    "print(\"Probes in first 1m lines of RIPE Dataset: \" +str(len(RIPEProbes)))            \n",
    "\n",
    "decomFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probes in both RIPE and probe dataset: 7221\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of probes with an ASN with type hosting: 194\n"
     ]
    }
   ],
   "source": [
    "HostProbeASNs = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for ASN in AS_df[\"ASN\"]:\n",
    "    \n",
    "    if ASN in ProbeASNs:\n",
    "        if AS_df[\"type\"][index] == \"hosting\":\n",
    "            HostProbeASNs.append(ASN)            \n",
    "    index = index + 1    \n",
    "    \n",
    "print(\"Amount of probes with an ASN with type hosting: \" + str(len(HostProbeASNs))) \n",
    "\n",
    "HostProbeASNs.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Answer to question B by Joost:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bz2 file:\n",
      "Loading 1m lines took: 68.42 seconds\n",
      "Estimated loading time of entire bz2 file: 1368 seconds\n"
     ]
    }
   ],
   "source": [
    "bz2Filename = 'data/ping-2022-03-01T0000.bz2'\n",
    "bz2File     = bz2.open(bz2Filename, 'rt') \n",
    "\n",
    "# List of tuples\n",
    "# https://stackoverflow.com/questions/28056171/how-to-build-and-fill-pandas-dataframe-from-for-loop\n",
    "tuple_list = []\n",
    "#read first 10 lines to estimate total loading time\n",
    "totalNrOfLines = 0;\n",
    "filteredNrOfLines = 0\n",
    "     \n",
    "start  = time.time()\n",
    "for line in bz2File:\n",
    "    #print(line)\n",
    "    decoded_line = json.loads(line)\n",
    "    if \"af\" in decoded_line and \"dst_addr\" in decoded_line and \"prb_id\" in decoded_line and \"avg\" in decoded_line: \n",
    "        filteredNrOfLines += 1\n",
    "        tuple_list.append((decoded_line[\"dst_addr\"],decoded_line[\"prb_id\"],decoded_line[\"avg\"]))\n",
    "    #print(decoded_line)\n",
    "    totalNrOfLines += 1\n",
    "    if totalNrOfLines > 2000000:\n",
    "        break\n",
    "#\n",
    "#print time and estimate total time            \n",
    "dur         = round(time.time() - start,2)\n",
    "estTotTime  = round( (dur/100000)*totalNrOfLines )\n",
    "print(\"\\nbz2 file:\" )\n",
    "print(\"Loading 1m lines took: \"  + str(dur) + \" seconds\")\n",
    "print(\"Estimated loading time of entire bz2 file: \"  + str(estTotTime) + \\\n",
    "      \" seconds\" )\n",
    "\n",
    "#finally close bz2File\n",
    "bz2File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_probe_joined_df = pd.merge(probe_df,AS_df, on='ASN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EU country codes retrieved from: https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Country_codes\n",
    "EU_list = ['BE','BG','CZ','DK','DE','EE','IE','EL','ES','FR','HR','IT','CY','LV','LT','LU','HU','MT','NL','AT','PL','PT','RO','SI','SK','FI','SE']\n",
    "\n",
    "\n",
    "# Filter data set for AS's that can be used for hosting in the EU\n",
    "as_probe_joined_df = as_probe_joined_df.loc[(as_probe_joined_df['type'] == 'hosting') & (as_probe_joined_df['Country'].isin(EU_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of unique probe ID's in RIPE dataset\n",
    "unique_prbID = []\n",
    "for i in tuple_list:\n",
    "    if i[1] not in unique_prbID:\n",
    "        unique_prbID.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prb_id</th>\n",
       "      <th>ASN</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "      <th>NumIPs</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>92</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>6237</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>32445</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>989</td>\n",
       "      <td>AS25596</td>\n",
       "      <td>NL</td>\n",
       "      <td>Cambrium IT Services B.V.</td>\n",
       "      <td>18,432</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>4680</td>\n",
       "      <td>AS25596</td>\n",
       "      <td>NL</td>\n",
       "      <td>Cambrium IT Services B.V.</td>\n",
       "      <td>18,432</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10476</th>\n",
       "      <td>52596</td>\n",
       "      <td>AS25542</td>\n",
       "      <td>NL</td>\n",
       "      <td>Netspider Group B.V.</td>\n",
       "      <td>31,232</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10477</th>\n",
       "      <td>52603</td>\n",
       "      <td>AS47869</td>\n",
       "      <td>NL</td>\n",
       "      <td>Ellada Projects B.V. trading as Netrouting</td>\n",
       "      <td>38,400</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>52810</td>\n",
       "      <td>AS47692</td>\n",
       "      <td>AT</td>\n",
       "      <td>Nessus GmbH</td>\n",
       "      <td>30,720</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10589</th>\n",
       "      <td>54556</td>\n",
       "      <td>AS30823</td>\n",
       "      <td>DE</td>\n",
       "      <td>combahton GmbH</td>\n",
       "      <td>21,248</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664</th>\n",
       "      <td>1000183</td>\n",
       "      <td>AS51167</td>\n",
       "      <td>DE</td>\n",
       "      <td>Contabo GmbH</td>\n",
       "      <td>177,152</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        prb_id      ASN Country                                        Name  \\\n",
       "2451        92  AS12859      NL                                      BIT BV   \n",
       "2453      6237  AS12859      NL                                      BIT BV   \n",
       "2454     32445  AS12859      NL                                      BIT BV   \n",
       "2523       989  AS25596      NL                   Cambrium IT Services B.V.   \n",
       "2526      4680  AS25596      NL                   Cambrium IT Services B.V.   \n",
       "...        ...      ...     ...                                         ...   \n",
       "10476    52596  AS25542      NL                        Netspider Group B.V.   \n",
       "10477    52603  AS47869      NL  Ellada Projects B.V. trading as Netrouting   \n",
       "10498    52810  AS47692      AT                                 Nessus GmbH   \n",
       "10589    54556  AS30823      DE                              combahton GmbH   \n",
       "10664  1000183  AS51167      DE                                Contabo GmbH   \n",
       "\n",
       "        NumIPs     type  \n",
       "2451    66,816  hosting  \n",
       "2453    66,816  hosting  \n",
       "2454    66,816  hosting  \n",
       "2523    18,432  hosting  \n",
       "2526    18,432  hosting  \n",
       "...        ...      ...  \n",
       "10476   31,232  hosting  \n",
       "10477   38,400  hosting  \n",
       "10498   30,720  hosting  \n",
       "10589   21,248  hosting  \n",
       "10664  177,152  hosting  \n",
       "\n",
       "[234 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter the data set by only selecting the ASN's that have probes in the Ripe dataset\n",
    "final_df = as_probe_joined_df.loc[as_probe_joined_df['prb_id'].isin(unique_prbID)]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hosting location (Question C)\n",
    "For a single hour in the RIPE data set: find all valid entries where the probe has hosting type AS and\n",
    "the target IPv4 is from an EU country. Implement this in an efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "HostProbes = []\n",
    "\n",
    "for ProbeASN in HostProbeASNs:\n",
    "    \n",
    "    index = 0\n",
    "    for ASN in P_df[\"ASN\"]:\n",
    "        if ASN == ProbeASN:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "            break\n",
    "        index = index + 1\n",
    "        \n",
    "print(len(HostProbes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPs in the RIPE data connected to an AS of type Hosting: 1047\n"
     ]
    }
   ],
   "source": [
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "\n",
    "HostIPs = []\n",
    "index = 0\n",
    "\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    \n",
    "    if jsonline[\"prb_id\"] in HostProbes:\n",
    "        try:\n",
    "            #Check for duplicates\n",
    "            if jsonline[\"dst_addr\"] not in HostIPs:\n",
    "                #Check if IP is of type 4\n",
    "                if jsonline[\"af\"] == 4:\n",
    "                    HostIPs.append(jsonline[\"dst_addr\"])\n",
    "        except KeyError as err:\n",
    "            pass\n",
    "    \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "                              \n",
    "print(\"Amount of IPs in the RIPE data connected to an AS of type Hosting: \" + str(len(HostIPs)))\n",
    "\n",
    "decomFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047\n"
     ]
    }
   ],
   "source": [
    "#Converting the xxx.xxx.xxx.xxx format of the host IPs to integer\n",
    "#Needed for when comparing to the IPv4 dataset\n",
    "\n",
    "HostIPs_Integer = []\n",
    "\n",
    "\n",
    "for IPString in HostIPs:\n",
    "    IP_Splitstring = IPString.split(\".\") \n",
    " \n",
    "    HostIPs_Integer.append(int(IP_Splitstring[0]) * 16581375 + int(IP_Splitstring[1]) * 65025 + int(IP_Splitstring[2]) * 255 + int(IP_Splitstring[3]))  \n",
    "\n",
    "print(len(HostIPs_Integer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of country codes that are an EU member\n",
    "\n",
    "EU_Countries = [\"AT\",\n",
    "    \"BE\",\n",
    "    \"HR\",\n",
    "    \"CY\",\n",
    "    \"CZ\",\n",
    "    \"DK\",\n",
    "    \"EE\",\n",
    "    \"FI\",\n",
    "    \"FR\",\n",
    "    \"GR\",\n",
    "    \"DE\",\n",
    "    \"HU\",\n",
    "    \"IE\",\n",
    "    \"IT\",\n",
    "    \"LV\",\n",
    "    \"LT\",\n",
    "    \"LU\",\n",
    "    \"MT\",\n",
    "    \"NL\",\n",
    "    \"PL\",\n",
    "    \"PT\",\n",
    "    \"RO\",\n",
    "    \"SK\",\n",
    "    \"SI\",\n",
    "    \"ES\",\n",
    "    \"SE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88072507\n",
      "HU\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Compare IPv4 with HostIPs_Integer\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "ipv4_df.head()\n",
    "\n",
    "HostIPs_EU = []\n",
    "index = 0\n",
    "ipv4index = 0\n",
    "\n",
    "#Sorting the IP list so we can check from low to high IPs\n",
    "HostIPs_Integer.sort\n",
    "\n",
    "for IP_to in ipv4_df[\"ip_to\"]:\n",
    "    \n",
    "    while HostIPs_Integer[index] < IP_to:\n",
    "        if ipv4_df[\"country_code\"][ipv4index] in EU_Countries:\n",
    "            HostIPs_EU.append(HostIPs_Integer[index])\n",
    "            print(HostIPs_Integer[index])\n",
    "            print(ipv4_df[\"country_code\"][ipv4index])\n",
    "        index = index + 1\n",
    "        if index >= len(HostIPs_Integer):\n",
    "            break;\n",
    "    ipv4index = ipv4index + 1  \n",
    "    if index >= len(HostIPs_Integer):\n",
    "        break;\n",
    "                  \n",
    "\n",
    "print(len(HostIPs_EU))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_from</th>\n",
       "      <th>ip_to</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16777216</td>\n",
       "      <td>16777471</td>\n",
       "      <td>US</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16777472</td>\n",
       "      <td>16778239</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16778240</td>\n",
       "      <td>16779263</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16779264</td>\n",
       "      <td>16781311</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16781312</td>\n",
       "      <td>16785407</td>\n",
       "      <td>JP</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ip_from     ip_to country_code              country_name\n",
       "0  16777216  16777471           US  United States of America\n",
       "1  16777472  16778239           CN                     China\n",
       "2  16778240  16779263           AU                 Australia\n",
       "3  16779264  16781311           CN                     China\n",
       "4  16781312  16785407           JP                     Japan"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipv4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Latency (Question D)\n",
    "Move from using only an hour to the full day. It is advisable to store the raw results of each file. Then,\n",
    "using all processed files, calculate the average latency’s for each country-AS combination and store\n",
    "the results into one ncountries ×m matrix. If we could place one server in each country, what would the\n",
    "minimum average latency be for each country? (include in your report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Optimal server locations (Question E)\n",
    "Since we are only allowed to place four servers, determine the best four datacenters based on the total\n",
    "latency for all countries. Report your findings and your procedure to obtain them. Also include the\n",
    "average latency for each country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "... \n",
    "add code if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbes = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(len(HostProbeASNs))\n",
    "\n",
    "for ASN in P_df[\"ASN\"]:\n",
    "    if ASN in HostProbeASNs:\n",
    "        if P_df[\"prb_id\"][index] not in HostProbes:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "    index = index + 1\n",
    "\n",
    "print(len(HostProbes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0#Random stuff I didn't want to throw away yet\n",
    "#0Code for finding all host probes from EU in the dataset of one hour\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# open decompressed file\n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt') \n",
    "\n",
    "#read first line and print\n",
    "#firstLine = decomFile.readline();\n",
    "#print(firstLine)\n",
    "\n",
    "#the line appears to be json-formatted: pretty print json\n",
    "#firstLineJson = json.loads(firstLine)\n",
    "\n",
    "#read all lines of first file\n",
    "count = 0\n",
    "st    = time.time()\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    #print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "    count = count + 1\n",
    "    if count > 10000: \n",
    "          break\n",
    "\n",
    "#print the last line\n",
    "print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "\n",
    "#print the read duration\n",
    "dur         = round(time.time() - st,2)\n",
    "print(\"Loading took: \" + str(dur) + \" seconds\")\n",
    "print(\"The file had \" + str(count) + \"lines\")\n",
    "\n",
    "#finally close decomFile\n",
    "decomFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
