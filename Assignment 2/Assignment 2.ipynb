{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7c449a",
   "metadata": {},
   "source": [
    "# SEN163A - Fundamentals of Data Analytics\n",
    "# Assignment 2 - Large-scale Internet Data Analysis\n",
    "### Ir. Jacopo De Stefani - [J.deStefani@tudelft.nl](mailto:J.deStefani@tudelft.nl)\n",
    "### Joao Pizani Flor, M.Sc. - [J.p.pizaniflor@tudelft.nl](mailto:J.p.pizaniflor@tudelft.nl)\n",
    "\n",
    "### 05-03-2022\n",
    "## Group 2\n",
    "- Emmanuel M Boateng - '5617642'\n",
    "- Joost Oortwijn - '4593472'\n",
    "- Philip Busscher - ''4611993''\n",
    "- Floris Kool - ''4975243''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1beb9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook presents the results of our analysis of potential ASN's which can be used for hosting the mobile banking IT infrastructure for GNI Bank. Based on three datasets (described in chapter 1), we have idenified 4 locations which are, based on the available data, the best hosting options for GNI Bank. The indepth analysis consists of 4 parts and is shown in chapter 2. The final conclusion of our analysis is presented in chapter 3. In addition, we have identified different limitations to the available data which could decrease the usability of our recommendations in chapter 3, therefore these should be taken into account. These limitations are described in chapter 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1f8e1",
   "metadata": {},
   "source": [
    "# 1. Dataset description\n",
    "\n",
    "Short description of the 4 datasets used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ec478",
   "metadata": {},
   "source": [
    "## 1.1 Opening the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5152fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44140561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AS Dataset\n",
    "\n",
    "AS_Filename = 'data/AS_dataset.pkl'\n",
    "\n",
    "with open(AS_Filename, 'rb') as file:\n",
    "    \n",
    "    AS_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5585c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probe dataset\n",
    "\n",
    "Probe_Filename = 'data/probe_dataset.pkl'\n",
    "\n",
    "with open(Probe_Filename, 'rb') as file:\n",
    "    \n",
    "    P_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "282f9435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading took: 20.75 seconds\n",
      "Lines added to tuple: 601435\n"
     ]
    }
   ],
   "source": [
    "#Ripe dataset (Single file)\n",
    "\n",
    "#Option 1 decompressed file\n",
    "decomFilename = 'data/ping-2022-03-01T2300_decom'\n",
    "#decomFile     = open(decomFilename, 'rt')\n",
    "\n",
    "#Option 2 BZ2 file\n",
    "bz2Filename = 'D:/FoDa Data/ping-2022-03-01T0000.bz2'\n",
    "bz2File     = bz2.open(bz2Filename, 'rt')\n",
    "\n",
    "\n",
    "# List of tuples\n",
    "# https://stackoverflow.com/questions/28056171/how-to-build-and-fill-pandas-dataframe-from-for-loop\n",
    "tuple_list = []\n",
    "\n",
    "start  = time.time()\n",
    "i = 0\n",
    "\n",
    "#for line in bz2File:\n",
    "for line in bz2File:\n",
    "    \n",
    "    decoded_line = json.loads(line)\n",
    "    if \"af\" in decoded_line and \"dst_addr\" in decoded_line and \"prb_id\" in decoded_line and \"avg\" in decoded_line: \n",
    "        if decoded_line[\"af\"] == 4:\n",
    "            tuple_list.append((decoded_line[\"dst_addr\"],decoded_line[\"prb_id\"],decoded_line[\"avg\"]))\n",
    "            \n",
    "    i = i + 1\n",
    "    if i > 1000000:\n",
    "        break\n",
    "\n",
    "            \n",
    "dur         = round(time.time() - start,2)\n",
    "print(\"Loading took: \"  + str(dur) + \" seconds\")\n",
    "print(\"Lines added to tuple: \" + str(len(tuple_list)))\n",
    "\n",
    "#finally close bz2File\n",
    "bz2File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56bbc784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading took: 0.22 seconds\n"
     ]
    }
   ],
   "source": [
    "#Load tuples data into dataframe\n",
    "start  = time.time()\n",
    "\n",
    "RIPE_df = pd.DataFrame(tuple_list)\n",
    "\n",
    "dur         = round(time.time() - start,2)\n",
    "print(\"Loading took: \"  + str(dur) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "488f08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP 2 Location dataset\n",
    "\n",
    "IP_Filename = \"data/IP2LOCATION-LITE-DB1.CSV\"\n",
    "\n",
    "ipv4_df = pd.read_csv(IP_Filename)\n",
    "\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47c3ba",
   "metadata": {},
   "source": [
    "More detailed description of data if needed (Can also be after opening each dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448a884",
   "metadata": {},
   "source": [
    "## 1.2 Limitations in data (Question A)\n",
    "\n",
    "Evaluate if there are limitations in the provided datasets (AS and probe data set). If you find limitations, describe these and conjecture possible reasons, supported with data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18606f7f",
   "metadata": {},
   "source": [
    "### 1.2.1 Limitations in the AS and Probe dataset\n",
    "A limitation in the AS and probe dataset is the fact that according to the AS dataset there are 534 AS's that can be used for hosting in the EU. But when the AS data set is merged with the probe dataset the number of potential AS's has decreased to 339. An explanation for this is the fact that the probe dataset consists of significantly less AS's then the AS dataset (3652 compared to 60122). This can be seen as limitation as potential AS's are left out of the analysis eventhough these could be a interesting options for hosting the EU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb9b5f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AS_df.loc[(AS_df['type'] == 'hosting') & (AS_df['Country'].isin(EU_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2106b443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_probe_joined_df = pd.merge(P_df,AS_df, on='ASN')\n",
    "len(as_probe_joined_df.loc[(as_probe_joined_df['type'] == 'hosting') & (as_probe_joined_df['Country'].isin(EU_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0ab970a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AS55330     1\n",
       "AS31573     1\n",
       "AS200609    1\n",
       "AS200605    1\n",
       "AS51879     1\n",
       "           ..\n",
       "AS48290     1\n",
       "AS48295     1\n",
       "AS48303     1\n",
       "AS48360     1\n",
       "AS37485     1\n",
       "Name: ASN, Length: 60122, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AS_df['ASN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59d2d2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AS3320      351\n",
       "AS7922      334\n",
       "AS6830      333\n",
       "AS3215      243\n",
       "AS12322     210\n",
       "           ... \n",
       "AS8282        1\n",
       "AS12775       1\n",
       "AS199484      1\n",
       "AS199853      1\n",
       "AS49432       1\n",
       "Name: ASN, Length: 3652, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df['ASN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d5576",
   "metadata": {},
   "source": [
    "### 1.2.2 Limitation in the IP location dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b4417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a201acca",
   "metadata": {},
   "source": [
    "### 1.2.3 Limitations in the RIPE dataset\n",
    "\n",
    "When looking at the RIPE dataset, it was found that for some lines the IP destination addresses were missing. This was not the case for the Probe ID's and average round trip times as these were always included within the lines of the RIPE dataset. Below the number of missing values in the first 5 million lines are shown. When looking at the lines where the destination address is missing, it can be seen that these adresses are missing because there is no node or server name provided or known. Obviously this results in the lack of a destination address of that specific ping measurement. Due to the size of the available dataset this is not assumed as a limitation and the lines with missing destination address are simply skipped when opening the RIPE dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bz2Filename = 'data/ping-2022-03-01T0000.bz2'\n",
    "bz2File_limitation = bz2.open(bz2Filename, 'rt') \n",
    "missing_adres = 0\n",
    "missing_probeID = 0\n",
    "missing_avg = 0\n",
    "line_number = 0\n",
    "\n",
    "for line in bz2File_limitation:\n",
    "    decoded_line = json.loads(line)\n",
    "    line_number += 1\n",
    "    if \"dst_addr\" not in decoded_line: \n",
    "        missing_adres += 1\n",
    "      \n",
    "    if \"prb_id\" not in decoded_line: \n",
    "        missing_probeID += 1\n",
    "        \n",
    "    if \"avg\" not in decoded_line: \n",
    "        missing_avg += 1\n",
    "           \n",
    "    if line_number > 5000000:\n",
    "        print('There are', missing_adres, 'missing IP destination addresses in the first 5m lines of the RIPE dataset (for one hour)')\n",
    "        print('There are', missing_probeID, 'missing probe ID\\'s in the first 5m lines of the RIPE dataset (for one hour)')\n",
    "        print('There are', missing_avg, 'missing average round-trip time values in the first 5m lines of the RIPE dataset (for one hour)')\n",
    "        break\n",
    "        \n",
    "bz2File_limitation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afaad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bz2Filename = 'data/ping-2022-03-01T0000.bz2'\n",
    "bz2File_limitation = bz2.open(bz2Filename, 'rt') \n",
    "line_number = 0\n",
    "\n",
    "for line in bz2File_limitation:\n",
    "    decoded_line = json.loads(line)\n",
    "    line_number += 1\n",
    "    if \"dst_addr\" not in decoded_line: \n",
    "        print(\"Destination adress is missing in at line:\", line_number,':')\n",
    "        print (line)\n",
    "    \n",
    "    if line_number > 10000:\n",
    "        break\n",
    "\n",
    "bz2File_limitation.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15149a03",
   "metadata": {},
   "source": [
    "# 2 Analysis\n",
    "\n",
    "Short description of what is going to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbd4f1",
   "metadata": {},
   "source": [
    "## 2.1 AS (Question B)\n",
    "\n",
    "With the AS and probe data set, find the number m of AS’s that can be used for hosting in the EU\n",
    "and have probes in the RIPE data set. Sort the ASN’s in ascending order and include the first and last\n",
    "three in your report (number, name and country).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ff34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the AS and Probe datasets\n",
    "as_probe_joined_df = pd.merge(P_df,AS_df, on='ASN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b948f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EU country codes retrieved from: https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Country_codes\n",
    "EU_list = ['BE','BG','CZ','DK','DE','EE','IE','EL','ES','FR','HR','IT','CY','LV','LT','LU','HU','MT','NL','AT','PL','PT','RO','SI','SK','FI','SE']\n",
    "\n",
    "# Filter data set for AS's that can be used for hosting in the EU\n",
    "as_probe_joined_df = as_probe_joined_df.loc[(as_probe_joined_df['type'] == 'hosting') & (as_probe_joined_df['Country'].isin(EU_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010c4b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique probe IDs: 11543\n"
     ]
    }
   ],
   "source": [
    "#Get the unique number of probe IDs that are in the RIPE Data\n",
    "unique_prbID = RIPE_df[1].unique()\n",
    "\n",
    "print(\"Unique probe IDs: \" + str(len(unique_prbID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cd78240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of probes connected to AS that can be used for hosting in the EU and are in the RIPE dataset: 234\n"
     ]
    }
   ],
   "source": [
    "#Filter the data set by only selecting the ASN's that have probes in the Ripe dataset\n",
    "AS_Probe_RIPE_df = as_probe_joined_df.loc[as_probe_joined_df['prb_id'].isin(unique_prbID)]\n",
    "\n",
    "#Sort by ASN\n",
    "AS_Probe_RIPE_df.sort_values(by=['ASN']).sort_values(by=['ASN'])\n",
    "\n",
    "print(\"Number of probes connected to AS that can be used for hosting in the EU and are in the RIPE dataset: \" + str(len(AS_Probe_RIPE_df[\"ASN\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87c861cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of AS that can be used for hosting in the EU and are in the RIPE dataset: 113\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicate ASNs (Probes connected to same AS)\n",
    "display_df = AS_Probe_RIPE_df.drop_duplicates(subset=['ASN'])\n",
    "\n",
    "#Remove unused columns\n",
    "display_df = display_df.drop(columns=['prb_id', 'NumIPs', 'type'])\n",
    "\n",
    "#Print anwser to question B\n",
    "print(\"Number of AS that can be used for hosting in the EU and are in the RIPE dataset: \" + str(len(display_df[\"ASN\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91843c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASN</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>AS25596</td>\n",
       "      <td>NL</td>\n",
       "      <td>Cambrium IT Services B.V.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2844</th>\n",
       "      <td>AS15598</td>\n",
       "      <td>DE</td>\n",
       "      <td>QSC AG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ASN Country                       Name\n",
       "2451  AS12859      NL                     BIT BV\n",
       "2523  AS25596      NL  Cambrium IT Services B.V.\n",
       "2844  AS15598      DE                     QSC AG"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 3 probes\n",
    "display_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5994efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASN</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>AS47692</td>\n",
       "      <td>AT</td>\n",
       "      <td>Nessus GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10589</th>\n",
       "      <td>AS30823</td>\n",
       "      <td>DE</td>\n",
       "      <td>combahton GmbH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664</th>\n",
       "      <td>AS51167</td>\n",
       "      <td>DE</td>\n",
       "      <td>Contabo GmbH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ASN Country            Name\n",
       "10498  AS47692      AT     Nessus GmbH\n",
       "10589  AS30823      DE  combahton GmbH\n",
       "10664  AS51167      DE    Contabo GmbH"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Last 3 probes\n",
    "display_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f65fd8",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d61b4",
   "metadata": {},
   "source": [
    "## 2.2 Hosting location (Question C)\n",
    "For a single hour in the RIPE data set: find all valid entries where the probe has hosting type AS and\n",
    "the target IPv4 is from an EU country. Implement this in an efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d31427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with probe connected to an EU as with type hosting: 24896\n"
     ]
    }
   ],
   "source": [
    "#Selects all entries in RIPE data with probe connected to EU as of type hosting\n",
    "RIPE_HostAS_df = RIPE_df.loc[RIPE_df[1].isin(AS_Probe_RIPE_df['prb_id'])]\n",
    "\n",
    "print(\"Entries with probe connected to an EU as with type hosting: \" + str(len(RIPE_HostAS_df[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5074bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.154.79.254\n"
     ]
    }
   ],
   "source": [
    "#Convert IP strings to IP integers\n",
    "for i in RIPE_HostAS_df.index:\n",
    "    \n",
    "    IP_Splitstring = RIPE_HostAS_df[0][i].split(\".\") \n",
    "    RIPE_HostAS_df.at[i, 0] = int(IP_Splitstring[0]) * 16581375 + int(IP_Splitstring[1]) * 65025 + int(IP_Splitstring[2]) * 255 + int(IP_Splitstring[3])\n",
    "\n",
    "\n",
    "\n",
    "#Add Integer values of IP to dataframe\n",
    "#RIPE_HostAS_df[\"IP_Integer\"] = IPs_Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f063f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP addresses not included in IP2location dataset: 4\n",
      "IP addresses linked to country: 24896\n"
     ]
    }
   ],
   "source": [
    "#Sorting the IP lists so we can check from low to high IPs\n",
    "RIPE_HostAS_df = RIPE_HostAS_df.sort_values(by=[0])\n",
    "ipv4_df = ipv4_df.sort_values(by=[\"ip_from\"])\n",
    "\n",
    "Dest_Addr_Countries = []\n",
    "ripeindex = 0\n",
    "ipindex = 0\n",
    "\n",
    "#Check if there are IP addresses lower than included in the IP2Location dataset\n",
    "while RIPE_HostAS_df.iat[ripeindex, 0] < ipv4_df.at[ipindex, \"ip_from\"]:\n",
    "    ripeindex = ripeindex + 1\n",
    "    Dest_Addr_Countries.append(\"-\")\n",
    "\n",
    "print(\"IP addresses not included in IP2location dataset: \" + str(ripeindex))\n",
    "\n",
    "#Check for each range of IP addresses in the IP2Location dataset which dst_addr IPs are present\n",
    "#Break loop early if the length of the RIPE dataset is reached\n",
    "for ipindex in ipv4_df.index:\n",
    "    while RIPE_HostAS_df.iat[ripeindex, 0] >= ipv4_df.at[ipindex, \"ip_from\"] and RIPE_HostAS_df.iat[ripeindex, 0] <= ipv4_df.at[ipindex, \"ip_to\"]:\n",
    "        Dest_Addr_Countries.append(ipv4_df.at[ipindex, \"country_code\"])\n",
    "        ripeindex = ripeindex + 1\n",
    "        if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "            break\n",
    "    \n",
    "    if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "        break\n",
    "\n",
    "print(\"IP addresses linked to country: \" + str(len(Dest_Addr_Countries)))\n",
    "#Add list for destination address location to dataframe\n",
    "RIPE_HostAS_df[\"Country\"] = Dest_Addr_Countries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c285f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with probe connected to an EU AS with type hosting and destination address within EU: 133653\n"
     ]
    }
   ],
   "source": [
    "#Remove entries not in EU\n",
    "RIPE_HostAS_df = RIPE_HostAS_df.loc[RIPE_HostAS_df['Country'].isin(EU_list)]\n",
    "\n",
    "print(\"Entries with probe connected to an EU AS with type hosting and destination address within EU: \" + str(len(RIPE_HostAS_df[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2cecc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>26262</td>\n",
       "      <td>3.964487</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24655</th>\n",
       "      <td>16581376</td>\n",
       "      <td>21831</td>\n",
       "      <td>4.715625</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182742</th>\n",
       "      <td>16646656</td>\n",
       "      <td>1000013</td>\n",
       "      <td>12.297646</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319075</th>\n",
       "      <td>16646656</td>\n",
       "      <td>1000013</td>\n",
       "      <td>10.942383</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392390</th>\n",
       "      <td>36807144</td>\n",
       "      <td>6726</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192059</th>\n",
       "      <td>36807144</td>\n",
       "      <td>6667</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439497</th>\n",
       "      <td>36807144</td>\n",
       "      <td>6734</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55541</th>\n",
       "      <td>36807144</td>\n",
       "      <td>6509</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55540</th>\n",
       "      <td>36807144</td>\n",
       "      <td>6509</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567533</th>\n",
       "      <td>36807144</td>\n",
       "      <td>6726</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>AE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1          2 Country\n",
       "1            123    26262   3.964487       -\n",
       "24655   16581376    21831   4.715625       -\n",
       "182742  16646656  1000013  12.297646       -\n",
       "319075  16646656  1000013  10.942383       -\n",
       "392390  36807144     6726  -1.000000      AE\n",
       "192059  36807144     6667  -1.000000      AE\n",
       "439497  36807144     6734  -1.000000      AE\n",
       "55541   36807144     6509  -1.000000      AE\n",
       "55540   36807144     6509  -1.000000      AE\n",
       "567533  36807144     6726  -1.000000      AE"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RIPE_HostAS_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fa9cd",
   "metadata": {},
   "source": [
    "## Part C Alternative Approach reading all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "#decomFile     = bz2.open(decomFilename, 'rt')   \n",
    "merged_df = P_df.merge(AS_df)\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]\n",
    "EU_Hosting = EU_data[EU_data['type'] == 'hosting']\n",
    "\n",
    "\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "\n",
    "merged_df_sorted = merged_df.sort_values('AS')\n",
    "df_HostingAS = merged_df[merged_df['type'] == 'hosting']\n",
    "\n",
    "ipv4_df.head()\n",
    "tpl = ipv4_df.loc[:, 'ip_from':'ip_to'].apply(tuple, 1).tolist()\n",
    "idx = pandas.IntervalIndex.from_tuples(tpl, 'both')\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(0.000001)\n",
    "with open(decomFilename, 'rb') as file:\n",
    "    decomp = bz2.BZ2Decompressor()\n",
    "    residue = b''\n",
    "    total_lines = 0\n",
    "    m = 0\n",
    "    checked = []\n",
    "    #102400 Bytes = 102.4 KB (in decimal)\n",
    "    #102400 Bytes = 100 KB (in binary)\n",
    "    #Iterate over RIPE data in  100 KB chunks \n",
    "    for data in iter(lambda: file.read(100 * 1024), b''):\n",
    "        # process the raw data and  concatenate residual of the previous block \n",
    "        #to the beginning of the current raw data block\n",
    "        raw = residue + decomp.decompress(data) \n",
    "        residue = b''\n",
    "        ## process_data(current_block) => do the processing of the \n",
    "        ##current data block\n",
    "        current_block = raw.split(b'\\n')\n",
    "        if raw[-1] != b'\\n':\n",
    "            residue = current_block.pop() # last line could be incomplete\n",
    "        ##Process all data in the current block to check    \n",
    "        for items in current_block:\n",
    "            df_dict = json.loads(items.decode('utf-8'))\n",
    "            if ('dst_addr' in df_dict) and (df_dict['af'] == 4):# and (ip2int(df_dict['dst_addr'])>0:\n",
    "                ##convert to interger\n",
    "                df_ip = ip2int(df_dict['dst_addr'])\n",
    "                \n",
    "                if df_ip > 0: #and (df_dict['prb_id'] not in checked)): # certain lines have 0.0.0.0 IP\n",
    "                    loc = idx.get_loc(df_ip)\n",
    "                    if ((ipv4_df.loc[loc,'country_code'] in EU_Countries) and (df_dict['prb_id'] not in checked)):\n",
    "                        #if len(EU_Hosting[EU_Hosting['prb_id'] == df_dict['prb_id']])!=0:\n",
    "                            #print(df_HostingAS[df_HostingAS['prb_id'] == df_dict['prb_id']])\n",
    "                        m +=1 ## increment count\n",
    "                       ##create a list of probes that could be used later                     \n",
    "                        checked.append(df_dict['prb_id']) \n",
    "        total_lines += len(current_block)\n",
    "    total_lines += 1\n",
    "\n",
    "print(\"Total processing time: \",(time.time() - t0))\n",
    "print(\"Total number of probe entries with hosting type AS and EU target in RIPE is %i\" %(m))\n",
    "fi.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036634dc",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01270a7",
   "metadata": {},
   "source": [
    "## 2.3 Latency (Question D)\n",
    "Move from using only an hour to the full day. It is advisable to store the raw results of each file. Then,\n",
    "using all processed files, calculate the average latency’s for each country-AS combination and store\n",
    "the results into one ncountries ×m matrix. If we could place one server in each country, what would the\n",
    "minimum average latency be for each country? (include in your report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "04518d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BE</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>85.256193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BE</td>\n",
       "      <td>AS25596</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BE</td>\n",
       "      <td>AS15598</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BE</td>\n",
       "      <td>AS203953</td>\n",
       "      <td>12.915613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BE</td>\n",
       "      <td>AS51815</td>\n",
       "      <td>3.754592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1          2\n",
       "0  BE   AS12859  85.256193\n",
       "1  BE   AS25596  -1.000000\n",
       "2  BE   AS15598        NaN\n",
       "3  BE  AS203953  12.915613\n",
       "4  BE   AS51815   3.754592"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want a matrix of 26 countries * 113 ASNs (For a single file, should be more for 24 files)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "import socket\n",
    "import struct\n",
    "from ip2geotools.databases.noncommercial import HostIP #gets the country code from ip\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "#decomFile     = bz2.open(decomFilename, 'rt')   \n",
    "merged_df = P_df.merge(AS_df)\n",
    "probes = merged_df['prb_id'].tolist()\n",
    "print(type(probes))\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]\n",
    "EU_Hosting = EU_data[EU_data['type'] == 'hosting']\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "df_HostingAS = merged_df[merged_df['type'] == 'hosting']\n",
    "\n",
    "ipv4_df.head()\n",
    "tpl = ipv4_df.loc[:, 'ip_from':'ip_to'].apply(tuple, 1).tolist()\n",
    "idx = pandas.IntervalIndex.from_tuples(tpl, 'both')\n",
    "    \n",
    "l =pandas.date_range('2022-03-01', periods=24, freq='60min').strftime('data/ping-%Y-%m-%dT%H%M.bz2').tolist()\n",
    "#print(l)\n",
    "m = 0\n",
    "data = []\n",
    "df = []\n",
    "for dataset in l:\n",
    "    decomFilename = dataset\n",
    "    print(decomFilename)\n",
    "    tstamp = str(decomFilename.strip('.bz2')[-5:])\n",
    "    #print(tstamp)\n",
    "\n",
    "    with open(decomFilename, 'rb') as fi:\n",
    "        decomp = bz2.BZ2Decompressor()\n",
    "        residue = b''\n",
    "        total_lines = 0\n",
    "        \n",
    "        for data in iter(lambda: fi.read(100 * 1024), b''):\n",
    "            raw = residue + decomp.decompress(data) # process the raw data and  concatenate residual of the previous block to the beginning of the current raw data block\n",
    "            residue = b''\n",
    "            # process_data(current_block) => do the processing of the current data block\n",
    "            current_block = raw.split(b'\\n')\n",
    "            if raw[-1] != b'\\n':\n",
    "                residue = current_block.pop() # last line could be incomplete\n",
    "            for items in current_block:\n",
    "                df_dict = json.loads(items.decode('utf-8'))\n",
    "                if ('dst_addr' in df_dict and df_dict['dst_addr']!= '0.0.0.0' and df_dict['af'] == 4 and df_dict['avg']>0):\n",
    "                    df_ip = ip2int(df_dict['dst_addr'])\n",
    "                    loc = idx.get_loc(df_ip)\n",
    "                    if ((ipv4_df.loc[loc,'country_code'] in EU_Countries) and df_dict['prb_id'] in probes):\n",
    "                        res = {key: df_dict[key] for key in df_dict.keys()\n",
    "                                   & {'avg','prb_id'}}\n",
    "                        res['Country'] = ipv4_df.loc[loc,'country_code']\n",
    "                        res['t'] = tstamp\n",
    "                        #print(res)\n",
    "                        df_line = pandas.DataFrame.from_dict(res, orient='index')\n",
    "                        df.append(df_line)\n",
    "                \n",
    "            total_lines += len(current_block)\n",
    "        total_lines += 1\n",
    "        if total_lines > 1000:\n",
    "            print('i should take a break')\n",
    "            break\n",
    "            \n",
    "    df.groupby('Country', as_index=False)['avg'].mean() \n",
    "    m+=1\n",
    "    if m>=1:\n",
    "        print('Taking a break')\n",
    "        break\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuple list\n",
    "test1= []\n",
    "\n",
    "for country in EU_list:\n",
    "    \n",
    "    #Filter each country's ping values seperately into a dataframe\n",
    "    country_df = RIPE_HostAS_df.loc[RIPE_HostAS_df['Country'] == country]\n",
    "    \n",
    "    for ASN in display_df[\"ASN\"]:\n",
    "        \n",
    "        #Filter probe IDs for each seperate ASN\n",
    "        #There are more probes than ASs to calculate the average ping more accurately we use all probes\n",
    "        prb_df = AS_Probe_RIPE_df.loc[AS_Probe_RIPE_df['ASN'] == ASN]                            \n",
    "        \n",
    "        #Filter the ping data so it includes all probes from selected ASN and selected country\n",
    "        temp_df = country_df.loc[country_df[1].isin(prb_df['prb_id'])]\n",
    "        \n",
    "        #Create sum of all ASN - Country ping measurements\n",
    "        sumvalue = 0\n",
    "        i = 0\n",
    "        for pingvalue in temp_df[2]:\n",
    "            sumvalue = sumvalue + pingvalue\n",
    "            i = i+1\n",
    "        \n",
    "        #Check if there are ping measurements between AS - Country\n",
    "        #Calculate average when needed, enter '-' when no data available\n",
    "        if not i == 0:\n",
    "            average = sumvalue/i\n",
    "            test1.append((country, ASN, average))\n",
    "        else:\n",
    "            test1.append((country, ASN, np.nan))\n",
    "\n",
    "#Load tuple list into dataframe\n",
    "test1_df = pd.DataFrame(test1)        \n",
    "test1_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "96327e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_df.columns = ['Country','ASN','Average latency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e4a2f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_df = test1_df.iloc[:, 1:] # asn and latency\n",
    "df_groupby = test1_df.groupby('ASN')['Average latency'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5016861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dftesttest = np.zeros((len(df_groupby), len(df_groupby[0])))\n",
    "for i in range(len(df_groupby)):\n",
    "    for j in range(len(df_groupby[0])):\n",
    "        new_dftesttest[i,j] = df_groupby[i][j]\n",
    "\n",
    "df_groupby.index\n",
    "final_df = pd.DataFrame(new_dftesttest.transpose())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0e3c5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = list(df_groupby.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1720eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns=column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "878b2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['index']=EU_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "39531aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AS12676</th>\n",
       "      <th>AS12824</th>\n",
       "      <th>AS12859</th>\n",
       "      <th>AS12876</th>\n",
       "      <th>AS12993</th>\n",
       "      <th>AS13287</th>\n",
       "      <th>AS15401</th>\n",
       "      <th>AS15598</th>\n",
       "      <th>AS15685</th>\n",
       "      <th>AS15817</th>\n",
       "      <th>...</th>\n",
       "      <th>AS61211</th>\n",
       "      <th>AS62000</th>\n",
       "      <th>AS62282</th>\n",
       "      <th>AS62416</th>\n",
       "      <th>AS6724</th>\n",
       "      <th>AS8304</th>\n",
       "      <th>AS8315</th>\n",
       "      <th>AS8560</th>\n",
       "      <th>AS8893</th>\n",
       "      <th>AS9211</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>88.093683</td>\n",
       "      <td>17.702773</td>\n",
       "      <td>85.256193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.643335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>226.285196</td>\n",
       "      <td>60.362766</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.313670</td>\n",
       "      <td>13.343822</td>\n",
       "      <td>92.592569</td>\n",
       "      <td>83.259170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.875999</td>\n",
       "      <td>87.930196</td>\n",
       "      <td>33.442691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BG</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CZ</th>\n",
       "      <td>32.819706</td>\n",
       "      <td>27.023314</td>\n",
       "      <td>26.958519</td>\n",
       "      <td>6.447432</td>\n",
       "      <td>47.118373</td>\n",
       "      <td>44.372507</td>\n",
       "      <td>32.211990</td>\n",
       "      <td>13.965507</td>\n",
       "      <td>7.243503</td>\n",
       "      <td>32.663917</td>\n",
       "      <td>...</td>\n",
       "      <td>15.138536</td>\n",
       "      <td>50.525012</td>\n",
       "      <td>30.112869</td>\n",
       "      <td>60.593795</td>\n",
       "      <td>39.976036</td>\n",
       "      <td>113.530596</td>\n",
       "      <td>21.579390</td>\n",
       "      <td>32.521064</td>\n",
       "      <td>39.861434</td>\n",
       "      <td>11.843888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DK</th>\n",
       "      <td>34.891794</td>\n",
       "      <td>28.365377</td>\n",
       "      <td>38.888919</td>\n",
       "      <td>15.644184</td>\n",
       "      <td>36.195367</td>\n",
       "      <td>37.156879</td>\n",
       "      <td>33.930247</td>\n",
       "      <td>4.563901</td>\n",
       "      <td>12.173853</td>\n",
       "      <td>38.981792</td>\n",
       "      <td>...</td>\n",
       "      <td>25.813521</td>\n",
       "      <td>48.207860</td>\n",
       "      <td>27.269326</td>\n",
       "      <td>60.481178</td>\n",
       "      <td>47.970564</td>\n",
       "      <td>34.615074</td>\n",
       "      <td>12.643969</td>\n",
       "      <td>37.872808</td>\n",
       "      <td>44.412661</td>\n",
       "      <td>25.870933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>49.938872</td>\n",
       "      <td>47.639296</td>\n",
       "      <td>48.532796</td>\n",
       "      <td>76.007365</td>\n",
       "      <td>54.801323</td>\n",
       "      <td>16.256942</td>\n",
       "      <td>18.103846</td>\n",
       "      <td>5.549774</td>\n",
       "      <td>9.584655</td>\n",
       "      <td>45.951487</td>\n",
       "      <td>...</td>\n",
       "      <td>22.065525</td>\n",
       "      <td>58.350937</td>\n",
       "      <td>11.998714</td>\n",
       "      <td>72.510263</td>\n",
       "      <td>55.345841</td>\n",
       "      <td>58.742821</td>\n",
       "      <td>21.460551</td>\n",
       "      <td>46.526449</td>\n",
       "      <td>55.097224</td>\n",
       "      <td>30.355043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EE</th>\n",
       "      <td>18.173319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.366154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.789540</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.625998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.242222</td>\n",
       "      <td>23.667669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.875320</td>\n",
       "      <td>6.618570</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IE</th>\n",
       "      <td>289.156367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247.740473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.463974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.267857</td>\n",
       "      <td>...</td>\n",
       "      <td>160.178071</td>\n",
       "      <td>215.952162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>222.628365</td>\n",
       "      <td>296.038515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>291.453679</td>\n",
       "      <td>294.540788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EL</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ES</th>\n",
       "      <td>54.128359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.365201</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>72.030021</td>\n",
       "      <td>103.286695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.791166</td>\n",
       "      <td>...</td>\n",
       "      <td>53.363730</td>\n",
       "      <td>125.304564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.223453</td>\n",
       "      <td>62.043651</td>\n",
       "      <td>34.328491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.916970</td>\n",
       "      <td>60.400514</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>33.885222</td>\n",
       "      <td>36.411679</td>\n",
       "      <td>37.821143</td>\n",
       "      <td>28.527121</td>\n",
       "      <td>32.407633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.262632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.896014</td>\n",
       "      <td>33.802222</td>\n",
       "      <td>...</td>\n",
       "      <td>17.975812</td>\n",
       "      <td>32.537519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.218164</td>\n",
       "      <td>37.969703</td>\n",
       "      <td>31.517369</td>\n",
       "      <td>56.521297</td>\n",
       "      <td>31.768459</td>\n",
       "      <td>33.555731</td>\n",
       "      <td>10.150618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT</th>\n",
       "      <td>35.368202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.312480</td>\n",
       "      <td>23.065625</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859746</td>\n",
       "      <td>33.438808</td>\n",
       "      <td>...</td>\n",
       "      <td>19.926797</td>\n",
       "      <td>44.687364</td>\n",
       "      <td>7.257385</td>\n",
       "      <td>64.686476</td>\n",
       "      <td>37.393895</td>\n",
       "      <td>32.076082</td>\n",
       "      <td>6.161718</td>\n",
       "      <td>29.936946</td>\n",
       "      <td>39.717213</td>\n",
       "      <td>23.983264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LV</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HU</th>\n",
       "      <td>96.666580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.378307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.127169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.102612</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.898791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.258043</td>\n",
       "      <td>99.638858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.583708</td>\n",
       "      <td>98.212520</td>\n",
       "      <td>11.770114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NL</th>\n",
       "      <td>51.199181</td>\n",
       "      <td>46.597165</td>\n",
       "      <td>44.974283</td>\n",
       "      <td>133.136022</td>\n",
       "      <td>19.963991</td>\n",
       "      <td>47.466499</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.555424</td>\n",
       "      <td>...</td>\n",
       "      <td>32.421941</td>\n",
       "      <td>57.251114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.516246</td>\n",
       "      <td>60.461850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.222546</td>\n",
       "      <td>53.332401</td>\n",
       "      <td>30.101287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>28.082384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.932289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.749708</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.478087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.820359</td>\n",
       "      <td>34.382705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.752654</td>\n",
       "      <td>32.880044</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL</th>\n",
       "      <td>42.592808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.682794</td>\n",
       "      <td>38.472671</td>\n",
       "      <td>45.628968</td>\n",
       "      <td>24.376482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.914309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.403102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.088585</td>\n",
       "      <td>51.540497</td>\n",
       "      <td>150.337385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.462040</td>\n",
       "      <td>49.184591</td>\n",
       "      <td>5.803617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT</th>\n",
       "      <td>45.974410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.875156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.229953</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.056962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.680723</td>\n",
       "      <td>51.322198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.120484</td>\n",
       "      <td>51.033820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RO</th>\n",
       "      <td>77.420666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.831504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218.910218</td>\n",
       "      <td>58.800107</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.181187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.061040</td>\n",
       "      <td>88.650735</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.151351</td>\n",
       "      <td>82.802082</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FI</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE</th>\n",
       "      <td>44.960367</td>\n",
       "      <td>28.523756</td>\n",
       "      <td>48.038601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.575358</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.003865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.450583</td>\n",
       "      <td>48.960752</td>\n",
       "      <td>15.774091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.427012</td>\n",
       "      <td>48.914567</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          AS12676    AS12824     AS12859     AS12876    AS12993     AS13287  \\\n",
       "index                                                                         \n",
       "BE      88.093683  17.702773   85.256193         NaN  10.643335         NaN   \n",
       "BG      -1.000000        NaN   -1.000000         NaN        NaN         NaN   \n",
       "CZ      32.819706  27.023314   26.958519    6.447432  47.118373   44.372507   \n",
       "DK      34.891794  28.365377   38.888919   15.644184  36.195367   37.156879   \n",
       "DE      49.938872  47.639296   48.532796   76.007365  54.801323   16.256942   \n",
       "EE      18.173319        NaN   14.366154         NaN        NaN         NaN   \n",
       "IE     289.156367        NaN  247.740473         NaN        NaN  231.463974   \n",
       "EL            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "ES      54.128359        NaN   53.365201   -1.000000  72.030021  103.286695   \n",
       "FR      33.885222  36.411679   37.821143   28.527121  32.407633         NaN   \n",
       "HR            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "IT      35.368202        NaN   39.312480   23.065625  -1.000000         NaN   \n",
       "CY            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "LV      -1.000000        NaN   -1.000000         NaN        NaN         NaN   \n",
       "LT            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "LU            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "HU      96.666580        NaN   46.378307         NaN        NaN         NaN   \n",
       "MT            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "NL      51.199181  46.597165   44.974283  133.136022  19.963991   47.466499   \n",
       "AT      28.082384        NaN   41.932289         NaN        NaN         NaN   \n",
       "PL      42.592808        NaN   45.682794   38.472671  45.628968   24.376482   \n",
       "PT      45.974410        NaN   64.875156         NaN        NaN         NaN   \n",
       "RO      77.420666        NaN   98.831504         NaN        NaN         NaN   \n",
       "SI            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "SK            NaN        NaN         NaN         NaN        NaN         NaN   \n",
       "FI      -1.000000        NaN   -1.000000         NaN  -1.000000         NaN   \n",
       "SE      44.960367  28.523756   48.038601         NaN        NaN         NaN   \n",
       "\n",
       "         AS15401    AS15598     AS15685     AS15817  ...     AS61211  \\\n",
       "index                                                ...               \n",
       "BE           NaN        NaN  226.285196   60.362766  ...         NaN   \n",
       "BG           NaN        NaN         NaN   -1.000000  ...         NaN   \n",
       "CZ     32.211990  13.965507    7.243503   32.663917  ...   15.138536   \n",
       "DK     33.930247   4.563901   12.173853   38.981792  ...   25.813521   \n",
       "DE     18.103846   5.549774    9.584655   45.951487  ...   22.065525   \n",
       "EE           NaN        NaN         NaN   20.789540  ...         NaN   \n",
       "IE           NaN        NaN         NaN  200.267857  ...  160.178071   \n",
       "EL           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "ES           NaN        NaN         NaN   45.791166  ...   53.363730   \n",
       "FR     10.262632        NaN    9.896014   33.802222  ...   17.975812   \n",
       "HR           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "IT           NaN        NaN    0.859746   33.438808  ...   19.926797   \n",
       "CY           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "LV           NaN        NaN         NaN   -1.000000  ...         NaN   \n",
       "LT           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "LU           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "HU           NaN   5.127169         NaN   92.102612  ...         NaN   \n",
       "MT           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "NL     -1.000000        NaN         NaN   39.555424  ...   32.421941   \n",
       "AT           NaN        NaN         NaN   25.749708  ...         NaN   \n",
       "PL           NaN        NaN         NaN   42.914309  ...         NaN   \n",
       "PT           NaN        NaN         NaN   47.229953  ...         NaN   \n",
       "RO           NaN        NaN  218.910218   58.800107  ...         NaN   \n",
       "SI           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "SK           NaN        NaN         NaN         NaN  ...         NaN   \n",
       "FI           NaN        NaN         NaN   -1.000000  ...         NaN   \n",
       "SE           NaN        NaN         NaN   36.575358  ...         NaN   \n",
       "\n",
       "          AS62000    AS62282     AS62416      AS6724      AS8304     AS8315  \\\n",
       "index                                                                         \n",
       "BE      76.313670  13.343822   92.592569   83.259170         NaN        NaN   \n",
       "BG      -1.000000        NaN   -1.000000   -1.000000         NaN        NaN   \n",
       "CZ      50.525012  30.112869   60.593795   39.976036  113.530596  21.579390   \n",
       "DK      48.207860  27.269326   60.481178   47.970564   34.615074  12.643969   \n",
       "DE      58.350937  11.998714   72.510263   55.345841   58.742821  21.460551   \n",
       "EE      23.625998        NaN   54.242222   23.667669         NaN        NaN   \n",
       "IE     215.952162        NaN  222.628365  296.038515         NaN        NaN   \n",
       "EL            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "ES     125.304564        NaN   92.223453   62.043651   34.328491        NaN   \n",
       "FR      32.537519        NaN   61.218164   37.969703   31.517369  56.521297   \n",
       "HR            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "IT      44.687364   7.257385   64.686476   37.393895   32.076082   6.161718   \n",
       "CY            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "LV      -1.000000        NaN   -1.000000   -1.000000         NaN        NaN   \n",
       "LT            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "LU            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "HU     105.898791        NaN  125.258043   99.638858         NaN        NaN   \n",
       "MT            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "NL      57.251114        NaN   62.516246   60.461850         NaN        NaN   \n",
       "AT      33.478087        NaN   63.820359   34.382705         NaN        NaN   \n",
       "PL      57.403102        NaN   68.088585   51.540497  150.337385        NaN   \n",
       "PT      51.056962        NaN   52.680723   51.322198         NaN        NaN   \n",
       "RO      82.181187        NaN   90.061040   88.650735   -1.000000        NaN   \n",
       "SI            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "SK            NaN        NaN         NaN         NaN         NaN        NaN   \n",
       "FI      -1.000000        NaN   -1.000000   -1.000000         NaN        NaN   \n",
       "SE      82.003865        NaN   69.450583   48.960752   15.774091        NaN   \n",
       "\n",
       "           AS8560      AS8893     AS9211  \n",
       "index                                     \n",
       "BE      80.875999   87.930196  33.442691  \n",
       "BG      -1.000000   -1.000000  -1.000000  \n",
       "CZ      32.521064   39.861434  11.843888  \n",
       "DK      37.872808   44.412661  25.870933  \n",
       "DE      46.526449   55.097224  30.355043  \n",
       "EE      22.875320    6.618570        NaN  \n",
       "IE     291.453679  294.540788        NaN  \n",
       "EL            NaN         NaN        NaN  \n",
       "ES      73.916970   60.400514  -1.000000  \n",
       "FR      31.768459   33.555731  10.150618  \n",
       "HR            NaN         NaN        NaN  \n",
       "IT      29.936946   39.717213  23.983264  \n",
       "CY            NaN         NaN        NaN  \n",
       "LV      -1.000000   -1.000000        NaN  \n",
       "LT            NaN         NaN        NaN  \n",
       "LU            NaN         NaN        NaN  \n",
       "HU      94.583708   98.212520  11.770114  \n",
       "MT            NaN         NaN        NaN  \n",
       "NL      48.222546   53.332401  30.101287  \n",
       "AT      26.752654   32.880044        NaN  \n",
       "PL      45.462040   49.184591   5.803617  \n",
       "PT      54.120484   51.033820        NaN  \n",
       "RO      83.151351   82.802082        NaN  \n",
       "SI            NaN         NaN        NaN  \n",
       "SK            NaN         NaN        NaN  \n",
       "FI      -1.000000   -1.000000        NaN  \n",
       "SE      45.427012   48.914567        NaN  \n",
       "\n",
       "[27 rows x 113 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c901386",
   "metadata": {},
   "source": [
    "## Reading data for D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ffe3cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/FoDa Data/ping-2022-03-01T0000.bz2\n",
      "Added 286373 entries and 113 ASNs in 742.29 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RIPE_Filenames = pd.date_range('2022-03-01', periods=24, freq='60min').strftime('D:/FoDa Data/ping-%Y-%m-%dT%H%M.bz2').tolist()\n",
    "\n",
    "Complete_ASN_List = []\n",
    "Complete_RIPE_Entries_df = pd.DataFrame(\n",
    "                                        {\n",
    "                                            0: [],\n",
    "                                            1: [],\n",
    "                                            2: [],\n",
    "                                            'Country': []\n",
    "                                        })\n",
    "\n",
    "for filename in RIPE_Filenames:\n",
    "    #Read RIPE data\n",
    "    print(filename)\n",
    "    bz2File     = bz2.open(filename, 'rt')\n",
    "    \n",
    "    tuple_list = []\n",
    "    \n",
    "    start  = time.time()\n",
    "    i = 0\n",
    "    \n",
    "    for line in bz2File:\n",
    "        decoded_line = json.loads(line)\n",
    "        if \"af\" in decoded_line and \"dst_addr\" in decoded_line and \"prb_id\" in decoded_line and \"avg\" in decoded_line: \n",
    "            if decoded_line[\"af\"] == 4:\n",
    "                tuple_list.append((decoded_line[\"dst_addr\"],decoded_line[\"prb_id\"],decoded_line[\"avg\"]))\n",
    "        #i = i + 1\n",
    "        #if i > 1000000:\n",
    "        #    break;\n",
    "    \n",
    "    bz2File.close()\n",
    "    \n",
    "    RIPE_df = pd.DataFrame(tuple_list)\n",
    "    \n",
    "    dur         = round(time.time() - start,2)\n",
    "    print(\"Lines added: \" + str(len(tuple_list)) + \"Loading took: \" + str(dur) + \" seconds\")\n",
    "    \n",
    "    #Get list of ASNs\n",
    "    unique_prbID = RIPE_df[1].unique()\n",
    "    \n",
    "    AS_Probe_RIPE_df = as_probe_joined_df.loc[as_probe_joined_df['prb_id'].isin(unique_prbID)]\n",
    "    \n",
    "    unique_ASNs = AS_Probe_RIPE_df['ASN'].unique()\n",
    "    Complete_ASN_List.extend(unique_ASNs)\n",
    "    \n",
    "    \n",
    "    #Get RIPE entries with dst addr in eu\n",
    "    RIPE_HostAS_df = RIPE_df.loc[RIPE_df[1].isin(AS_Probe_RIPE_df['prb_id'])]\n",
    "    \n",
    "    for i in RIPE_HostAS_df.index:\n",
    "        RIPE_HostAS_df.at[i, 0] = int(ipaddress.IPv4Address(RIPE_HostAS_df[0][i]))\n",
    "\n",
    "    RIPE_HostAS_df = RIPE_HostAS_df.sort_values(by=[0])\n",
    "    ipv4_df = ipv4_df.sort_values(by=[\"ip_from\"])\n",
    "\n",
    "    Dest_Addr_Countries = []\n",
    "    ripeindex = 0\n",
    "    ipindex = 0\n",
    "\n",
    "    while RIPE_HostAS_df.iat[ripeindex, 0] < ipv4_df.at[ipindex, \"ip_from\"]:\n",
    "        ripeindex = ripeindex + 1\n",
    "        Dest_Addr_Countries.append(\"-\")\n",
    "\n",
    "    for ipindex in ipv4_df.index:\n",
    "        while RIPE_HostAS_df.iat[ripeindex, 0] >= ipv4_df.at[ipindex, \"ip_from\"] and RIPE_HostAS_df.iat[ripeindex, 0] <= ipv4_df.at[ipindex, \"ip_to\"]:\n",
    "            Dest_Addr_Countries.append(ipv4_df.at[ipindex, \"country_code\"])\n",
    "            ripeindex = ripeindex + 1\n",
    "            if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "                break\n",
    "\n",
    "        if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "            break\n",
    "            \n",
    "    RIPE_HostAS_df[\"Country\"] = Dest_Addr_Countries\n",
    "    RIPE_HostAS_df = RIPE_HostAS_df.loc[RIPE_HostAS_df['Country'].isin(EU_list)]    \n",
    "    \n",
    "    \n",
    "    #Add entries to complete dataframe\n",
    "    frames = [Complete_RIPE_Entries_df, RIPE_HostAS_df]\n",
    "    Complete_RIPE_Entries_df = pd.concat(frames)\n",
    "    \n",
    "    dur         = round(time.time() - start,2)\n",
    "    print(\"Added \" + str(len(RIPE_HostAS_df[0])) + \" entries and \" + str(len(unique_ASNs)) + \" ASNs in \" + str(dur) + \" seconds\")\n",
    "    print()\n",
    "    \n",
    "    break\n",
    "\n",
    "#Remove duplicates\n",
    "Complete_ASN_Set = set(Complete_ASN_List)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f39334",
   "metadata": {},
   "source": [
    "## Same code, but reading data raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78812350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/FoDa Data/ping-2022-03-01T0000.bz2\n",
      "Lines added: 17099133Loading took: 531.3 seconds\n",
      "Added 286373 entries and 113 ASNs in 584.04 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Changed 24 -> 6\n",
    "RIPE_Filenames = pd.date_range('2022-03-01', periods=24, freq='60min').strftime('D:/FoDa Data/ping-%Y-%m-%dT%H%M.bz2').tolist()\n",
    "\n",
    "Complete_ASN_List = []\n",
    "Complete_RIPE_Entries_df = pd.DataFrame({0:[], 1:[], 2:[], 'Country':[]})\n",
    "\n",
    "\n",
    "for filename in RIPE_Filenames:\n",
    "    #Read RIPE data\n",
    "    print(filename)\n",
    "    \n",
    "    start  = time.time()\n",
    "    with open(filename, 'rb') as fi:\n",
    "        decomp = bz2.BZ2Decompressor()\n",
    "        residue = b''\n",
    "        total_lines = 0\n",
    "        m = 0\n",
    "        tuple_list = []\n",
    "    \n",
    "        for data in iter(lambda: fi.read(100 * 1024), b''):\n",
    "            raw = residue + decomp.decompress(data) # process the raw data and  concatenate residual of the previous block to the beginning of the current raw data block\n",
    "            residue = b''\n",
    "            # process_data(current_block) => do the processing of the current data block\n",
    "            current_block = raw.split(b'\\n')\n",
    "            if raw[-1] != b'\\n':\n",
    "                residue = current_block.pop() # last line could be incomplete\n",
    "\n",
    "            for items in current_block:\n",
    "                df_dict = json.loads(items.decode('utf-8'))\n",
    "                if ('dst_addr' in df_dict) and (df_dict['af'] == 4):\n",
    "                    tuple_list.append((df_dict[\"dst_addr\"],df_dict[\"prb_id\"],df_dict[\"avg\"]))\n",
    "    \n",
    "    fi.close()\n",
    "    \n",
    "    RIPE_df = pd.DataFrame(tuple_list)\n",
    "    \n",
    "    #Get list of ASNs\n",
    "    unique_prbID = RIPE_df[1].unique()\n",
    "    \n",
    "    AS_Probe_RIPE_df = as_probe_joined_df.loc[as_probe_joined_df['prb_id'].isin(unique_prbID)]\n",
    "    \n",
    "    unique_ASNs = AS_Probe_RIPE_df['ASN'].unique()\n",
    "    Complete_ASN_List.extend(unique_ASNs)\n",
    "    \n",
    "    \n",
    "    #Get RIPE entries with dst addr in eu\n",
    "    RIPE_HostAS_df = RIPE_df.loc[RIPE_df[1].isin(AS_Probe_RIPE_df['prb_id'])]\n",
    "    \n",
    "    for i in RIPE_HostAS_df.index:\n",
    "        RIPE_HostAS_df.at[i, 0] = int(ipaddress.IPv4Address(RIPE_HostAS_df[0][i]))\n",
    "\n",
    "    RIPE_HostAS_df = RIPE_HostAS_df.sort_values(by=[0])\n",
    "    ipv4_df = ipv4_df.sort_values(by=[\"ip_from\"])\n",
    "\n",
    "    Dest_Addr_Countries = []\n",
    "    ripeindex = 0\n",
    "    ipindex = 0\n",
    "\n",
    "    while RIPE_HostAS_df.iat[ripeindex, 0] < ipv4_df.at[ipindex, \"ip_from\"]:\n",
    "        ripeindex = ripeindex + 1\n",
    "        Dest_Addr_Countries.append(\"-\")\n",
    "\n",
    "    for ipindex in ipv4_df.index:\n",
    "        while RIPE_HostAS_df.iat[ripeindex, 0] >= ipv4_df.at[ipindex, \"ip_from\"] and RIPE_HostAS_df.iat[ripeindex, 0] <= ipv4_df.at[ipindex, \"ip_to\"]:\n",
    "            Dest_Addr_Countries.append(ipv4_df.at[ipindex, \"country_code\"])\n",
    "            ripeindex = ripeindex + 1\n",
    "            if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "                break\n",
    "\n",
    "        if ripeindex >= len(RIPE_HostAS_df[0]):\n",
    "            break\n",
    "            \n",
    "    RIPE_HostAS_df[\"Country\"] = Dest_Addr_Countries\n",
    "    RIPE_HostAS_df = RIPE_HostAS_df.loc[RIPE_HostAS_df['Country'].isin(EU_list)]    \n",
    "    \n",
    "    \n",
    "    #Add entries to complete dataframe\n",
    "    frames = [Complete_RIPE_Entries_df, RIPE_HostAS_df]\n",
    "    Complete_RIPE_Entries_df = pd.concat(frames)\n",
    "    \n",
    "    dur         = round(time.time() - start,2)\n",
    "    print(\"Added \" + str(len(RIPE_HostAS_df[0])) + \" entries and \" + str(len(unique_ASNs)) + \" ASNs in \" + str(dur) + \" seconds\")\n",
    "    print()\n",
    "    \n",
    "    break\n",
    "\n",
    "#Remove duplicates\n",
    "Complete_ASN_Set = set(Complete_ASN_List)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "936d5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete_ASN_Set_df = pd.DataFrame(Complete_ASN_Set)\n",
    "\n",
    "Complete_ASN_Set_df.to_pickle(\"D:/FoDa Data/ASN_00.pkl\")\n",
    "\n",
    "Complete_RIPE_Entries_df.to_pickle(\"D:/FoDa Data/RIPE_00.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a62f8ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "237089\n"
     ]
    }
   ],
   "source": [
    "print(len(Complete_ASN_Set))\n",
    "print(len(Complete_RIPE_Entries_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68bbf9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:/FoDa Data/ping-2022-03-01T0000.bz2',\n",
       " 'D:/FoDa Data/ping-2022-03-01T0100.bz2',\n",
       " 'D:/FoDa Data/ping-2022-03-01T0200.bz2',\n",
       " 'D:/FoDa Data/ping-2022-03-01T0300.bz2',\n",
       " 'D:/FoDa Data/ping-2022-03-01T0400.bz2',\n",
       " 'D:/FoDa Data/ping-2022-03-01T0500.bz2']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipaddress\n",
    "\n",
    "int(ipaddress.IPv4Address('192.168.82.0'))\n",
    "\n",
    "test_list = []\n",
    "\n",
    "value = 10\n",
    "\n",
    "for i in range(value):\n",
    "    test = [i]\n",
    "    test[0] = i \n",
    "    test.append(i) \n",
    "    test.append(i)\n",
    "    test_list.extend(test)\n",
    "\n",
    "print(test_list)\n",
    "\n",
    "test_set = set(test_list)\n",
    "\n",
    "print(test_set)\n",
    "print()\n",
    "\n",
    "test_total_df = pd.DataFrame({0:[],\n",
    "                              \"b\":[]})\n",
    "for i in range(value):\n",
    "    test = {0: [i, i],\n",
    "            \"b\": [(i*10),(i*10)]}\n",
    "    test_df = pd.DataFrame(test)\n",
    "    frames = [test_total_df, test_df]\n",
    "    test_total_df = pd.concat(frames)\n",
    "\n",
    "test_total_df\n",
    "\n",
    "RIPE_Filenames = pd.date_range('2022-03-01', periods=6, freq='60min').strftime('D:/FoDa Data/ping-%Y-%m-%dT%H%M.bz2').tolist()\n",
    "RIPE_Filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7a5f5",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee912e63",
   "metadata": {},
   "source": [
    "## 2.4 Optimal server locations (Question E)\n",
    "Since we are only allowed to place four servers, determine the best four datacenters based on the total\n",
    "latency for all countries. Report your findings and your procedure to obtain them. Also include the\n",
    "average latency for each country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e91f33",
   "metadata": {},
   "source": [
    "0Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2837d87",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "... \n",
    "add code if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c9b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce70c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "HostProbes = []\n",
    "\n",
    "for ProbeASN in HostProbeASNs:\n",
    "    \n",
    "    index = 0\n",
    "    for ASN in P_df[\"ASN\"]:\n",
    "        if ASN == ProbeASN:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "            break\n",
    "        index = index + 1\n",
    "        \n",
    "print(len(HostProbes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c09103d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPs in the RIPE data connected to an AS of type Hosting: 1047\n"
     ]
    }
   ],
   "source": [
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "\n",
    "HostIPs = []\n",
    "index = 0\n",
    "\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    \n",
    "    if jsonline[\"prb_id\"] in HostProbes:\n",
    "        try:\n",
    "            #Check for duplicates\n",
    "            if jsonline[\"dst_addr\"] not in HostIPs:\n",
    "                #Check if IP is of type 4\n",
    "                if jsonline[\"af\"] == 4:\n",
    "                    HostIPs.append(jsonline[\"dst_addr\"])\n",
    "        except KeyError as err:\n",
    "            pass\n",
    "    \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "                              \n",
    "print(\"Amount of IPs in the RIPE data connected to an AS of type Hosting: \" + str(len(HostIPs)))\n",
    "\n",
    "decomFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "232d9b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047\n"
     ]
    }
   ],
   "source": [
    "#Converting the xxx.xxx.xxx.xxx format of the host IPs to integer\n",
    "#Needed for when comparing to the IPv4 dataset\n",
    "\n",
    "HostIPs_Integer = []\n",
    "\n",
    "\n",
    "for IPString in HostIPs:\n",
    "    IP_Splitstring = IPString.split(\".\") \n",
    " \n",
    "    HostIPs_Integer.append(int(IP_Splitstring[0]) * 16581375 + int(IP_Splitstring[1]) * 65025 + int(IP_Splitstring[2]) * 255 + int(IP_Splitstring[3]))  \n",
    "\n",
    "print(len(HostIPs_Integer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea313f63",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     P_df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     10\u001b[0m decomFilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m decomFile     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecomFilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     14\u001b[0m RIPEProbes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'"
     ]
    }
   ],
   "source": [
    "#Upper part should be removed because run in part 1\n",
    "import pickle\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "\n",
    "RIPEProbes = []\n",
    "index = 0\n",
    "\n",
    "#Create list of all probes that are in the RIPE dataset\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    if jsonline[\"prb_id\"] not in RIPEProbes:\n",
    "        RIPEProbes.append(jsonline[\"prb_id\"])\n",
    "                          \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "        \n",
    "                  \n",
    "print(\"Probes in first 1m lines of RIPE Dataset: \" +str(len(RIPEProbes)))            \n",
    "\n",
    "decomFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1c28648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probes in both RIPE and probe dataset: 7221\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7933600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of country codes that are an EU member\n",
    "\n",
    "EU_Countries = [\"AT\",\n",
    "    \"BE\",\n",
    "    \"HR\",\n",
    "    \"CY\",\n",
    "    \"CZ\",\n",
    "    \"DK\",\n",
    "    \"EE\",\n",
    "    \"FI\",\n",
    "    \"FR\",\n",
    "    \"GR\",\n",
    "    \"DE\",\n",
    "    \"HU\",\n",
    "    \"IE\",\n",
    "    \"IT\",\n",
    "    \"LV\",\n",
    "    \"LT\",\n",
    "    \"LU\",\n",
    "    \"MT\",\n",
    "    \"NL\",\n",
    "    \"PL\",\n",
    "    \"PT\",\n",
    "    \"RO\",\n",
    "    \"SK\",\n",
    "    \"SI\",\n",
    "    \"ES\",\n",
    "    \"SE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d319f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of probes with an ASN with type hosting: 194\n"
     ]
    }
   ],
   "source": [
    "HostProbeASNs = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for ASN in AS_df[\"ASN\"]:\n",
    "    \n",
    "    if ASN in ProbeASNs:\n",
    "        if AS_df[\"type\"][index] == \"hosting\":\n",
    "            HostProbeASNs.append(ASN)            \n",
    "    index = index + 1    \n",
    "    \n",
    "print(\"Amount of probes with an ASN with type hosting: \" + str(len(HostProbeASNs))) \n",
    "\n",
    "HostProbeASNs.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4278f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88072507\n",
      "HU\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Compare IPv4 with HostIPs_Integer\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "ipv4_df.head()\n",
    "\n",
    "HostIPs_EU = []\n",
    "index = 0\n",
    "ipv4index = 0\n",
    "\n",
    "#Sorting the IP list so we can check from low to high IPs\n",
    "HostIPs_Integer.sort\n",
    "\n",
    "for IP_to in ipv4_df[\"ip_to\"]:\n",
    "    \n",
    "    while HostIPs_Integer[index] < IP_to:\n",
    "        if ipv4_df[\"country_code\"][ipv4index] in EU_Countries:\n",
    "            HostIPs_EU.append(HostIPs_Integer[index])\n",
    "            print(HostIPs_Integer[index])\n",
    "            print(ipv4_df[\"country_code\"][ipv4index])\n",
    "        index = index + 1\n",
    "        if index >= len(HostIPs_Integer):\n",
    "            break;\n",
    "    ipv4index = ipv4index + 1  \n",
    "    if index >= len(HostIPs_Integer):\n",
    "        break;\n",
    "                  \n",
    "\n",
    "print(len(HostIPs_EU))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of unique probe ID's in RIPE dataset\n",
    "unique_prbID = []\n",
    "for i in tuple_list:\n",
    "    if i[1] not in unique_prbID:\n",
    "        unique_prbID.append(i[1])\n",
    "        \n",
    "print(\"Unique probe IDs in RIPE dataset: \" + str(len(unique_prbID.l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6da4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbes = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(len(HostProbeASNs))\n",
    "\n",
    "for ASN in P_df[\"ASN\"]:\n",
    "    if ASN in HostProbeASNs:\n",
    "        if P_df[\"prb_id\"][index] not in HostProbes:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "    index = index + 1\n",
    "\n",
    "print(len(HostProbes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5419100",
   "metadata": {},
   "outputs": [],
   "source": [
    "0#Random stuff I didn't want to throw away yet\n",
    "#0Code for finding all host probes from EU in the dataset of one hour\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# open decompressed file\n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt') \n",
    "\n",
    "#read first line and print\n",
    "#firstLine = decomFile.readline();\n",
    "#print(firstLine)\n",
    "\n",
    "#the line appears to be json-formatted: pretty print json\n",
    "#firstLineJson = json.loads(firstLine)\n",
    "\n",
    "#read all lines of first file\n",
    "count = 0\n",
    "st    = time.time()\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    #print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "    count = count + 1\n",
    "    if count > 10000: \n",
    "          break\n",
    "\n",
    "#print the last line\n",
    "print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "\n",
    "#print the read duration\n",
    "dur         = round(time.time() - st,2)\n",
    "print(\"Loading took: \" + str(dur) + \" seconds\")\n",
    "print(\"The file had \" + str(count) + \"lines\")\n",
    "\n",
    "#finally close decomFile\n",
    "decomFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
