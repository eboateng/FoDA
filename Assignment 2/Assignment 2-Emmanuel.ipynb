{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bce53b7",
   "metadata": {},
   "source": [
    "# SEN163A - Fundamentals of Data Analytics\n",
    "# Assignment 2 - Large-scale Internet Data Analysis\n",
    "### Ir. Jacopo De Stefani - [J.deStefani@tudelft.nl](mailto:J.deStefani@tudelft.nl)\n",
    "### Joao Pizani Flor, M.Sc. - [J.p.pizaniflor@tudelft.nl](mailto:J.p.pizaniflor@tudelft.nl)\n",
    "\n",
    "### 05-03-2022\n",
    "## Group 2\n",
    "- Emmanuel M Boateng - '5617642'\n",
    "- Joost Oortwijn - '4593472'\n",
    "- Philip Busscher - ''4611993''\n",
    "- Floris Kool - ''4975243''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14223a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "short description of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa652d22",
   "metadata": {},
   "source": [
    "# 1. Dataset description\n",
    "\n",
    "Short description of the 4 datasets used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417f273",
   "metadata": {},
   "source": [
    "## 1.1 Opening the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1251d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AS Dataset\n",
    "\n",
    "AS_Filename = 'data/AS_dataset.pkl'\n",
    "\n",
    "with open(AS_Filename, 'rb') as file:\n",
    "    \n",
    "    AS_df = pickle.load(file)\n",
    "\n",
    "AS_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probe dataset\n",
    "\n",
    "Probe_Filename = 'data/probe_dataset.pkl'\n",
    "\n",
    "with open(Probe_Filename, 'rb') as file:\n",
    "    \n",
    "    P_df = pickle.load(file)\n",
    "\n",
    "P_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb11fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ripe dataset \n",
    "\n",
    "\n",
    "#Method needs to be decided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5c5c4-0730-46b1-81be-35cacd4fbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = P_df.merge(AS_df)\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "\n",
    "merged_df = merged_df.sort_values('AS')\n",
    "merged_df.head(3)\n",
    "\n",
    "print(merged_df.type.value_counts().hosting)\n",
    "#print(merged_df.loc[(merged_df['type']=='') | (merged_df['Country']==\"\")])\n",
    "\n",
    "#merged_df.info('NumIPs')\n",
    "#& (df['FT_Team'].str.startswith('S')),['Name','FT_Team']]\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b31ac8-e525-4ce6-82c3-6865fbb0370d",
   "metadata": {},
   "source": [
    "the first three ASNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066df344-43f2-47cf-911e-b5a85b39c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd23ce-99a0-4872-8cd3-5aaf8a4577ce",
   "metadata": {},
   "source": [
    "the last three ASNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db435c8e-223c-49f0-a0a9-2f87a2324154",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP 2 Location dataset\n",
    "\n",
    "IP_Filename = \"data/IP2LOCATION-LITE-DB1.CSV\"\n",
    "\n",
    "ipv4_df = pandas.read_csv(IP_Filename)\n",
    "\n",
    "ipv4_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a36dc",
   "metadata": {},
   "source": [
    "More detailed description of data if needed (Can also be after opening each dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148358f4",
   "metadata": {},
   "source": [
    "## 1.2 Limitations in data\n",
    "\n",
    "Evaluate if there are limitations in the provided datasets (AS and probe data set). If you find limitations, describe these and conjecture possible reasons, supported with data.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code needed to prove limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aaef1e-8c31-4c32-8955-b79c9afc99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bz2Filename = 'data/ping-2022-03-01T2300.bz2'\n",
    "bz2File     = bz2.open(bz2Filename, 'rt') \n",
    "firstLine = bz2File.readline();\n",
    "firstLine_sizeInBytes   = sys.getsizeof(firstLine)\n",
    "print(\"size of line 1\",firstLine_sizeInBytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeb0f5",
   "metadata": {},
   "source": [
    "Some list of limitations in text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0f1dd",
   "metadata": {},
   "source": [
    "# 2 Analysis\n",
    "\n",
    "Short description of what is going to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96172fcb",
   "metadata": {},
   "source": [
    "## 2.1 AS\n",
    "\n",
    "With the AS and probe data set, find the number m of AS’s that can be used for hosting in the EU\n",
    "and have probes in the RIPE data set. Sort the ASN’s in ascending order and include the first and last\n",
    "three in your report (number, name and country).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upper part should be removed because run in part 1\n",
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "#decomFile     = bz2.open(decomFilename, 'rt')   \n",
    "merged_df = P_df.merge(AS_df)\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "\n",
    "merged_df = merged_df.sort_values('AS')\n",
    "merged_df.head(3)\n",
    "\n",
    "#print(merged_df.type.value_counts().hosting)\n",
    "#print(merged_df.loc[(merged_df['type']=='') | (merged_df['Country']==\"\")])\n",
    "\n",
    "#merged_df.info('NumIPs')\n",
    "#& (df['FT_Team'].str.startswith('S')),['Name','FT_Team']]\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]\n",
    "EU_Hosting = EU_data[EU_data['type'] == 'hosting']\n",
    "\n",
    "\n",
    "rmv_list =['result','fw','mver','lts','af','proto','size','dup','rcvd','sent','min', 'ttl','max','msm_id','msm_name','step']\n",
    "#Create list of all probes that are in the RIPE dataset\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(0.000001)\n",
    "with open(decomFilename, 'rb') as fi:\n",
    "    decomp = bz2.BZ2Decompressor()\n",
    "    residue = b''\n",
    "    total_lines = 0\n",
    "    m = 0\n",
    "    checked = []\n",
    "    for data in iter(lambda: fi.read(100 * 1024), b''):\n",
    "        raw = residue + decomp.decompress(data) # process the raw data and  concatenate residual of the previous block to the beginning of the current raw data block\n",
    "        residue = b''\n",
    "        # process_data(current_block) => do the processing of the current data block\n",
    "        current_block = raw.split(b'\\n')\n",
    "        if raw[-1] != b'\\n':\n",
    "            residue = current_block.pop() # last line could be incomplete\n",
    "            \n",
    "        for items in current_block:\n",
    "            df_dict = json.loads(items.decode('utf-8'))\n",
    "            if (df_dict['prb_id'] in EU_Hosting[\"prb_id\"]) and (df_dict['af'] == 4):\n",
    "                if df_dict['prb_id'] not in checked:\n",
    "                    m +=1\n",
    "                    checked.append(df_dict['prb_id'])\n",
    "                    #print(checked)\n",
    "        total_lines += len(current_block)\n",
    "    total_lines += 1\n",
    "    #print('Final: %i lines/sec' % (total_lines / (time.time() - t0)))\n",
    "#print(data)\n",
    "#print(\"*****\")\n",
    "#print(current_block)\n",
    "print(\"Total processing time: \",(time.time() - t0))\n",
    "print(\"total lines :\", total_lines)\n",
    "print(\"Total number of EU hosting probes in RIPE is %i\" %(m))\n",
    "fi.close()\n",
    "#######-------good code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367f1e0-3ef6-4eec-92ea-56aaaff52fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "#decomFile     = bz2.open(decomFilename, 'rt')   \n",
    "merged_df = P_df.merge(AS_df)\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]\n",
    "EU_Hosting = EU_data[EU_data['type'] == 'hosting']\n",
    "\n",
    "\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "\n",
    "merged_df_sorted = merged_df.sort_values('AS')\n",
    "df_HostingAS = merged_df[merged_df['type'] == 'hosting']\n",
    "\n",
    "ipv4_df.head()\n",
    "tpl = ipv4_df.loc[:, 'ip_from':'ip_to'].apply(tuple, 1).tolist()\n",
    "idx = pandas.IntervalIndex.from_tuples(tpl, 'both')\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(0.000001)\n",
    "with open(decomFilename, 'rb') as file:\n",
    "    decomp = bz2.BZ2Decompressor()\n",
    "    residue = b''\n",
    "    total_lines = 0\n",
    "    m = 0\n",
    "    checked = []\n",
    "    #102400 Bytes = 102.4 KB (in decimal)\n",
    "    #102400 Bytes = 100 KB (in binary)\n",
    "    #Iterate over RIPE data in  100 KB chunks \n",
    "    for data in iter(lambda: file.read(100 * 1024), b''):\n",
    "        # process the raw data and  concatenate residual of the previous block \n",
    "        #to the beginning of the current raw data block\n",
    "        raw = residue + decomp.decompress(data) \n",
    "        residue = b''\n",
    "        ## process_data(current_block) => do the processing of the \n",
    "        ##current data block\n",
    "        current_block = raw.split(b'\\n')\n",
    "        if raw[-1] != b'\\n':\n",
    "            residue = current_block.pop() # last line could be incomplete\n",
    "        ##Process all data in the current block to check    \n",
    "        for items in current_block:\n",
    "            df_dict = json.loads(items.decode('utf-8'))\n",
    "            if ('dst_addr' in df_dict) and (df_dict['af'] == 4):# and (ip2int(df_dict['dst_addr'])>0:\n",
    "                ##convert to interger\n",
    "                df_ip = ip2int(df_dict['dst_addr'])\n",
    "                #print(df_ip)\n",
    "                if df_ip > 0: # certain lines have 0.0.0.0 IP\n",
    "                    loc = idx.get_loc(df_ip)\n",
    "                    if ((ipv4_df.loc[loc,'country_code'] in EU_Countries) and (df_dict['prb_id'] not in checked)):\n",
    "                        #if len(EU_Hosting[EU_Hosting['prb_id'] == df_dict['prb_id']])!=0:\n",
    "                            #print(df_HostingAS[df_HostingAS['prb_id'] == df_dict['prb_id']])\n",
    "                        m +=1 ## increment count\n",
    "                       ##create a list of probes that could be used later                     \n",
    "                        checked.append(df_dict['prb_id']) \n",
    "        total_lines += len(current_block)\n",
    "    total_lines += 1\n",
    "\n",
    "print(\"Total processing time: \",(time.time() - t0))\n",
    "print(\"Total number of probe entries with hosting type AS and EU target in RIPE is %i\" %(m))\n",
    "fi.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6d70ac-5a4e-47f6-bbfd-e892c9314999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prb_id</th>\n",
       "      <th>ASN</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "      <th>NumIPs</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>57</td>\n",
       "      <td>AS20621</td>\n",
       "      <td>DE</td>\n",
       "      <td>PlusServer GmbH</td>\n",
       "      <td>10,240</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>92</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>6234</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>6237</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>32445</td>\n",
       "      <td>AS12859</td>\n",
       "      <td>NL</td>\n",
       "      <td>BIT BV</td>\n",
       "      <td>66,816</td>\n",
       "      <td>hosting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prb_id      ASN Country             Name  NumIPs     type\n",
       "1637     57  AS20621      DE  PlusServer GmbH  10,240  hosting\n",
       "2451     92  AS12859      NL           BIT BV  66,816  hosting\n",
       "2452   6234  AS12859      NL           BIT BV  66,816  hosting\n",
       "2453   6237  AS12859      NL           BIT BV  66,816  hosting\n",
       "2454  32445  AS12859      NL           BIT BV  66,816  hosting"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "with open('data/AS_dataset.pkl', 'rb') as file:\n",
    "    AS_df = pickle.load(file)\n",
    "    \n",
    "with open('data/probe_dataset.pkl', 'rb') as file:    \n",
    "    P_df = pickle.load(file)\n",
    "    \n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "#decomFile     = bz2.open(decomFilename, 'rt')   \n",
    "merged_df = P_df.merge(AS_df)\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "\n",
    "EU_Countries = [\"AT\",\"BE\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"GR\",\"DE\",\"HU\",\n",
    "                \"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\n",
    "                \"ES\",\"SE\"]\n",
    "\n",
    "EU_data = merged_df[merged_df['Country'].isin(EU_Countries)]\n",
    "EU_Hosting = EU_data[EU_data['type'] == 'hosting']\n",
    "\n",
    "\n",
    "\n",
    "merged_df.insert(2, 'AS', merged_df['ASN'].str.replace('AS',''))\n",
    "merged_df['AS'] = pandas.to_numeric(merged_df['AS'])\n",
    "merged_df['prb_id'] = pandas.to_numeric(merged_df['prb_id'])\n",
    "\n",
    "\n",
    "merged_df_sorted = merged_df.sort_values('AS')\n",
    "df_HostingAS = merged_df[merged_df['type'] == 'hosting']\n",
    "EU_Hosting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcefb74f-d2fd-435b-88d3-322bf65f447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv4_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c98dbe-415a-445c-9a7e-be62b29f6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "####-----------------------\n",
    "#### Code to exclusively read the whole ripe file\n",
    "#####-----------------------\n",
    "import pickle\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(0.000001)\n",
    "with open(decomFilename, 'rb') as fi:\n",
    "    decomp = bz2.BZ2Decompressor()\n",
    "    residue = b''\n",
    "    total_lines = 0\n",
    "    for data in iter(lambda: fi.read(100 * 1024), b''):\n",
    "        raw = residue + decomp.decompress(data) # process the raw data and  concatenate residual of the previous block to the beginning of the current raw data block\n",
    "        residue = b''\n",
    "        # process_data(current_block) => do the processing of the current data block\n",
    "        current_block = raw.split(b'\\n')\n",
    "        if raw[-1] != b'\\n':\n",
    "            residue = current_block.pop() # last line could be incomplete\n",
    "        total_lines += len(current_block)\n",
    "    total_lines += 1\n",
    "    #print('Final: %i lines/sec' % (total_lines / (time.time() - t0)))\n",
    "#print(data)\n",
    "#print(\"*****\")\n",
    "#print(current_block)\n",
    "print(\"Total processing time: \",(time.time() - t0))\n",
    "print(\"total lines :\", total_lines)\n",
    "fi.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553c9c5-ebb2-4915-bd84-d94506979bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1982715-7ac4-494a-93d7-f8e0cd8ad9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =raw.splitlines()\n",
    "\n",
    "rmv_list =['result','fw','mver','ttr','lts','af','proto','size','dup','rcvd','sent','min', 'ttl','max','msm_id','msm_name','step']\n",
    "count = 0\n",
    "RIPE_df = pandas.DataFrame()  \n",
    "for items in data:\n",
    "    count = count +1\n",
    "    df_dict = json.loads(items.decode('utf-8'))\n",
    "    for items in rmv_list:\n",
    "        if items in df_dict:\n",
    "            del df_dict[items]\n",
    "            df = pandas.DataFrame([df_dict], columns=df_dict.keys())\n",
    "    RIPE_df = pandas.concat([RIPE_df, df], axis =0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0eb403-a30f-41f8-91af-9e1c58d53385",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIPE_df.tail()\n",
    "#print(RIPE_df.groupby('prb_id').count())\n",
    "print(\"Number of probes in Ripe:\",RIPE_df['prb_id'].value_counts().sum())\n",
    "print(\"Number of probes in Probe:\", P_df['prb_id'].value_counts().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139c548-5846-4671-8024-47a8c923c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    index = index +1\n",
    "    d = datetime.datetime.utcfromtimestamp(jsonline[\"timestamp\"])\n",
    "    if d.minute <= 30:\n",
    "        if jsonline[\"prb_id\"] not in RIPEProbes:\n",
    "            for items in rmv_list:\n",
    "                if items in jsonline:\n",
    "                    jsonline.pop(items)\n",
    "            RIPEProbes.append(jsonline)\n",
    "    if index == nrOfLines_half:\n",
    "        break\n",
    "                          \n",
    "    #Read only first 1m lines\n",
    "    #index = index + 1\n",
    "    #if index > 200:#0000:\n",
    "    #    break\n",
    "                  \n",
    "print(\"Probes in first 1m lines of RIPE Dataset: \" +str(len(RIPEProbes)))            \n",
    "ripe_df = pandas.DataFrame(RIPEProbes)\n",
    "ripe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead64bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbeASNs = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for ASN in AS_df[\"ASN\"]:\n",
    "    \n",
    "    if ASN in ProbeASNs:\n",
    "        if AS_df[\"type\"][index] == \"hosting\":\n",
    "            HostProbeASNs.append(ASN)            \n",
    "    index = index + 1    \n",
    "    \n",
    "print(\"Amount of probes with an ASN with type hosting: \" + str(len(HostProbeASNs))) \n",
    "\n",
    "HostProbeASNs.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61518",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5c14a",
   "metadata": {},
   "source": [
    "## 2.2 Hosting location\n",
    "For a single hour in the RIPE data set: find all valid entries where the probe has hosting type AS and\n",
    "the target IPv4 is from an EU country. Implement this in an efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6146fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbes = []\n",
    "\n",
    "for ProbeASN in HostProbeASNs:\n",
    "    \n",
    "    index = 0\n",
    "    for ASN in P_df[\"ASN\"]:\n",
    "        if ASN == ProbeASN:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "            break\n",
    "        index = index + 1\n",
    "        \n",
    "print(len(HostProbes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt')   \n",
    "\n",
    "HostIPs = []\n",
    "index = 0\n",
    "\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    \n",
    "    \n",
    "    if jsonline[\"prb_id\"] in HostProbes:\n",
    "        try:\n",
    "            #Check for duplicates\n",
    "            if jsonline[\"dst_addr\"] not in HostIPs:\n",
    "                #Check if IP is of type 4\n",
    "                if jsonline[\"af\"] == 4:\n",
    "                    HostIPs.append(jsonline[\"dst_addr\"])\n",
    "        except KeyError as err:\n",
    "            pass\n",
    "    \n",
    "    #Read only first 1m lines\n",
    "    index = index + 1\n",
    "    if index > 1000000:\n",
    "        break\n",
    "                              \n",
    "print(\"Amount of IPs in the RIPE data connected to an AS of type Hosting: \" + str(len(HostIPs)))\n",
    "\n",
    "decomFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0355671-cbd9-4dee-b605-4aa3c88c5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emm\n",
    "\n",
    "decomFilename = 'data/ping-2022-03-01T2300.bz2'\n",
    "decomFile     = open(decomFilename, 'rb') \n",
    "bz2File     = bz2.open(decomFilename, 'rb') \n",
    "\n",
    "firstLine = decomFile.readline()\n",
    "\n",
    "firstLine_sizeInBytes   = sys.getsizeof(firstLine) \n",
    "decomFile_sizeInBytes   = os.stat(decomFilename).st_size\n",
    "nrOfLines               = round(decomFile_sizeInBytes/firstLine_sizeInBytes)\n",
    "print(\"\\nEstimated nr of lines = \" + str(nrOfLines))\n",
    "\n",
    "\n",
    "count = 0;\n",
    "st    = time.time()\n",
    "for line in bz2File:\n",
    "    count = count + 1\n",
    "    if count>100000: \n",
    "        break\n",
    "    \n",
    "#print time and estimate total time            \n",
    "dur         = round(time.time() - st,2)\n",
    "estTotTime  = round( (dur/100000)*nrOfLines )\n",
    "print(\"\\nDecompressed file:\" )\n",
    "print(\"Loading 100k lines took: \"  + str(dur) + \" seconds\")\n",
    "print(\"Estimated loading time of entire decompression file: \"  + str(estTotTime) + \" seconds\" )\n",
    "\n",
    "#---------------------\n",
    "#Do needed conversion\n",
    "#---------------------\n",
    "\n",
    "print('-------')\n",
    "datarow = []\n",
    "counter = 0\n",
    "encoding = 'utf-8'\n",
    "keys = ['af','avg', 'dst_addr','dst_name','from','prb_id','src_addr','timestamp','type']\n",
    "st    = time.time()\n",
    "\n",
    "for line in bz2File:\n",
    "    counter = counter +1\n",
    "    data = \n",
    "    rmv_list =['result','fw','mver','lts','af','proto','size','dup','rcvd','sent','min', 'ttl','max','msm_id','msm_name','step']\n",
    "    print(data)\n",
    "    for items in rmv_list:\n",
    "        if items in data:\n",
    "            data.pop(items)\n",
    "    datarow.append(data)\n",
    "    if counter == 1000000:\n",
    "        break\n",
    "    \n",
    "dur         = round(time.time() - st,2)\n",
    "#estTotTime  = round( (dur/100000)*nrOfLines )\n",
    "#print(\"\\nDecompressed file:\" )\n",
    "print(\"\\nLoading lines took: \"  + str(dur) + \" seconds\")\n",
    "#print(\"\\nEstimated loading time of entire decompression file: \"  + str(estTotTime) + \" seconds\" )\n",
    "        \n",
    "df = pandas.DataFrame(datarow)\n",
    "decomFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138c431-1260-43fd-a274-81ed6eeb6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in decomFile:\n",
    "    data = json.loads(line)\n",
    "    print(type(data))\n",
    "    res_dict = data.pop('result')\n",
    "    datarow.append(res_dict)\n",
    "    #counter = counter +1\n",
    "    #if counter == 100000:\n",
    "    #    print(line)\n",
    "     #   break\n",
    "        \n",
    "        for line in decomFile:\n",
    "    counter = counter +1\n",
    "    if counter == 1:\n",
    "        print(line)\n",
    "        #dataform = str(response_json).strip(\"'<>() \").replace('\\'', '\\\"')\n",
    "        data = line.decode(encoding, errors= 'ignore')\n",
    "        firstLineJson = json.loads(data)\n",
    "        print(json.dumps(firstLineJson, sort_keys=True, indent=4))\n",
    "        break\n",
    "\n",
    "for line in bz2File:\n",
    "    counter = counter +1\n",
    "    data = json.loads(line)\n",
    "    rmv_list =['result','fw','mver','lts','af','proto','size','dup','rcvd','sent','min', 'ttl','max','msm_id','msm_name','step']\n",
    "    for items in rmv_list:\n",
    "        if items in data:\n",
    "            data.pop(items)\n",
    "    datarow.append(data)\n",
    "    #if counter == 1:\n",
    "    #    print(data)\n",
    "    #    break\n",
    "    \n",
    "dur         = round(time.time() - st,2)\n",
    "#estTotTime  = round( (dur/100000)*nrOfLines )\n",
    "#print(\"\\nDecompressed file:\" )\n",
    "print(\"\\nLoading lines took: \"  + str(dur) + \" seconds\")\n",
    "#print(\"\\nEstimated loading time of entire decompression file: \"  + str(estTotTime) + \" seconds\" )\n",
    "        \n",
    "df = pandas.DataFrame(datarow)\n",
    "decomFile.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6e92e-fe9e-43fb-9d96-681d1da61e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f62781-57d3-4d53-89e2-ca3a62d6901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = b'{\"fw\":5020,\"mver\":\"2.2.1\",\"lts\":-1,\"dst_name\":\"192.5.5.241\",\"af\":4,\"dst_addr\":\"192.5.5.241\",\"src_addr\":\"172.17.0.2\",\"proto\":\"ICMP\",\"ttl\":58,\"size\":32,\"result\":[{\"rtt\":13.444936},{\"rtt\":3.762035},{\"rtt\":3.557907}],\"dup\":0,\"rcvd\":3,\"sent\":3,\"min\":3.557907,\"max\":13.444936,\"avg\":6.921626,\"msm_id\":1004,\"prb_id\":1001660,\"timestamp\":1646176375,\"msm_name\":\"Ping\",\"from\":\"83.243.73.91\",\"type\":\"ping\",\"step\":240}\\n'\n",
    "\n",
    "dict_str = sdata.decode(\"UTF-8\")\n",
    "my_data = ast.literal_eval(dict_str)\n",
    "type(my_data)\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46da52-b384-4a0d-b2a4-bbe7c0615420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Driver code\n",
    "dict_1 = {'John': 15, 'Rick': 10, 'Misa' : 12 }\n",
    "dict_2 = {'John': 18,'Rick': 20,'Misa' : 16 }\n",
    "\n",
    "case_list = [dict_1]\n",
    "\n",
    "case_list.append(dict_2)\n",
    "print(case_list)\n",
    "\n",
    "df1 = pandas.DataFrame(case_list)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d276c9-9564-491a-8f75-3737c36ab698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the xxx.xxx.xxx.xxx format of the host IPs to integer\n",
    "#Needed for when comparing to the IPv4 dataset\n",
    "\n",
    "HostIPs_Integer = []\n",
    "\n",
    "\n",
    "for IPString in HostIPs:\n",
    "    IP_Splitstring = IPString.split(\".\") \n",
    " \n",
    "    HostIPs_Integer.append(int(IP_Splitstring[0]) * 16581375 + int(IP_Splitstring[1]) * 65025 + int(IP_Splitstring[2]) * 255 + int(IP_Splitstring[3]))  \n",
    "\n",
    "print(len(HostIPs_Integer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cfa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of country codes that are an EU member\n",
    "\n",
    "EU_Countries = [\"AT\",\n",
    "    \"BE\",\n",
    "    \"HR\",\n",
    "    \"CY\",\n",
    "    \"CZ\",\n",
    "    \"DK\",\n",
    "    \"EE\",\n",
    "    \"FI\",\n",
    "    \"FR\",\n",
    "    \"GR\",\n",
    "    \"DE\",\n",
    "    \"HU\",\n",
    "    \"IE\",\n",
    "    \"IT\",\n",
    "    \"LV\",\n",
    "    \"LT\",\n",
    "    \"LU\",\n",
    "    \"MT\",\n",
    "    \"NL\",\n",
    "    \"PL\",\n",
    "    \"PT\",\n",
    "    \"RO\",\n",
    "    \"SK\",\n",
    "    \"SI\",\n",
    "    \"ES\",\n",
    "    \"SE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare IPv4 with HostIPs_Integer\n",
    "\n",
    "ipv4_df = pandas.read_csv(\"data/IP2LOCATION-LITE-DB1.CSV\")\n",
    "ipv4_df.rename(columns = {'0':'ip_from', '16777215':'ip_to',\n",
    "                              '-':'country_code','-.1':'country_name'}, inplace = True)\n",
    "\n",
    "ipv4_df.head()\n",
    "\n",
    "HostIPs_EU = []\n",
    "index = 0\n",
    "ipv4index = 0\n",
    "\n",
    "#Sorting the IP list so we can check from low to high IPs\n",
    "HostIPs_Integer.sort\n",
    "\n",
    "for IP_to in ipv4_df[\"ip_to\"]:\n",
    "    \n",
    "    while HostIPs_Integer[index] < IP_to:\n",
    "        if ipv4_df[\"country_code\"][ipv4index] in EU_Countries:\n",
    "            HostIPs_EU.append(HostIPs_Integer[index])\n",
    "            print(HostIPs_Integer[index])\n",
    "            print(ipv4_df[\"country_code\"][ipv4index])\n",
    "        index = index + 1\n",
    "        if index >= len(HostIPs_Integer):\n",
    "            break;\n",
    "    ipv4index = ipv4index + 1  \n",
    "    if index >= len(HostIPs_Integer):\n",
    "        break;\n",
    "                  \n",
    "\n",
    "print(len(HostIPs_EU))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e762519",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipv4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d032c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2191f3fa",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd58bd1",
   "metadata": {},
   "source": [
    "## 2.3 Latency\n",
    "Move from using only an hour to the full day. It is advisable to store the raw results of each file. Then,\n",
    "using all processed files, calculate the average latency’s for each country-AS combination and store\n",
    "the results into one $n_{countries}$ × $m$ matrix. If we could place one server in each country, what would the\n",
    "minimum average latency be for each country? (include in your report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88c47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code...\n",
    "#Create a list of all the datasets to loop through\n",
    "\n",
    "l =pandas.date_range('2022-03-01', periods=24, freq='60min').strftime('data/ping-%Y-%m-%dT%H%M.bz2').tolist()\n",
    "print(l)\n",
    "\n",
    "\n",
    "d = {}\n",
    "for dataset in l:\n",
    "    decomFilename = dataset\n",
    "    tstamp = str(decomFilename.strip('.bz2')[-5:])\n",
    "    \n",
    "    with open(decomFilename, 'rb') as fi:\n",
    "        decomp = bz2.BZ2Decompressor()\n",
    "        residue = b''\n",
    "        total_lines = 0\n",
    "    for data in iter(lambda: fi.read(100 * 1024), b''):\n",
    "        raw = residue + decomp.decompress(data) # process the raw data and  concatenate residual of the previous block to the beginning of the current raw data block\n",
    "        residue = b''\n",
    "        # process_data(current_block) => do the processing of the current data block\n",
    "        current_block = raw.split(b'\\n')\n",
    "        if raw[-1] != b'\\n':\n",
    "            residue = current_block.pop() # last line could be incomplete\n",
    "        for items in current_block:\n",
    "            df_dict = json.loads(items.decode('utf-8'))\n",
    "            d[tstamp] = d[tstamp].append(df_dict)\n",
    "        total_lines += len(current_block)\n",
    "    total_lines += 1\n",
    "    \n",
    "print(\"Total processing time: \",(time.time() - t0))\n",
    "print(\"total lines :\", total_lines)\n",
    "fi.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ef015-e61f-43d0-b69f-067c51381183",
   "metadata": {},
   "outputs": [],
   "source": [
    "l =pandas.date_range('2022-03-01', periods=24, freq='60min').strftime('data/ping-%Y-%m-%dT%H%M.bz2').tolist()\n",
    "#print(l)\n",
    "\n",
    "d = {}\n",
    "for dataset in l:\n",
    "    decomFilename = dataset\n",
    "    tstamp = str(decomFilename.strip('.bz2')[-5:])\n",
    "    print(tstamp)\n",
    "\n",
    "    with open(decomFilename, 'rb') as fi:\n",
    "        decomp = bz2.BZ2Decompressor()\n",
    "        residue = b''\n",
    "        total_lines = 0\n",
    "        d[tstamp] = []\n",
    "        for data in iter(lambda: fi.read(100 * 1024), b''):\n",
    "            raw = residue + decomp.decompress(data) # process the raw data and  concatenate residual of the previous block to the beginning of the current raw data block\n",
    "            residue = b''\n",
    "            # process_data(current_block) => do the processing of the current data block\n",
    "            current_block = raw.split(b'\\n')\n",
    "            if raw[-1] != b'\\n':\n",
    "                residue = current_block.pop() # last line could be incomplete\n",
    "            for items in current_block:\n",
    "                df_dict = json.loads(items.decode('utf-8'))\n",
    "                if (df_dict['af'] == 4):\n",
    "                    res = {key: df_dict[key] for key in df_dict.keys()\n",
    "                               & {'af', 'avg', 'from','dst_addr','prb_id'}}\n",
    "                    \n",
    "                    d[tstamp].append(res)\n",
    "                    \n",
    "            #print(len(d))\n",
    "            #print(sys.getsizeof(d))\n",
    "            #print(d[tstamp])\n",
    "            #print(\"length is \", len(d[tstamp]))\n",
    "            #print(\"size is \",sys.getsizeof(d[tstamp]))\n",
    "break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a330946-67f7-4542-8789-3310e85708dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary1 = {  'Pen': 5, 'Pencil': 4, 'Chocolate' : 15 }\n",
    "dictionary2 = {'Apple': 25,'Ball': 10,'Doll' : 20 }\n",
    "dictionary1.update(dictionary2)\n",
    "print(dictionary1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b3e60",
   "metadata": {},
   "source": [
    "Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30083e90",
   "metadata": {},
   "source": [
    "## 2.4 Optimal server locations\n",
    "Since we are only allowed to place four servers, determine the best four datacenters based on the total\n",
    "latency for all countries. Report your findings and your procedure to obtain them. Also include the\n",
    "average latency for each country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a903acb",
   "metadata": {},
   "source": [
    "0Description of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713386f",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "... \n",
    "add code if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ddae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b213d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "ProbeASNs = []\n",
    "\n",
    "#Create list of all probes in both RIPE and probe datasets\n",
    "#Saves only the ASNs as these are used later\n",
    "#Probe IDs no longer used after this point\n",
    "for probe in P_df[\"prb_id\"]:\n",
    "    if probe in RIPEProbes:\n",
    "        ProbeASNs.append(P_df[\"ASN\"][index])\n",
    "        \n",
    "    index = index + 1\n",
    "    \n",
    "print(\"Probes in both RIPE and probe dataset: \" + str(len(ProbeASNs)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20864ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HostProbes = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(len(HostProbeASNs))\n",
    "\n",
    "for ASN in P_df[\"ASN\"]:\n",
    "    if ASN in HostProbeASNs:\n",
    "        if P_df[\"prb_id\"][index] not in HostProbes:\n",
    "            HostProbes.append(P_df[\"prb_id\"][index])\n",
    "    index = index + 1\n",
    "\n",
    "print(len(HostProbes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "0#Random stuff I didn't want to throw away yet\n",
    "#0Code for finding all host probes from EU in the dataset of one hour\n",
    "import time\n",
    "import bz2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# open decompressed file\n",
    "decomFilename = 'C:/Users/Kooltje/Downloads/FoDa Data/ping-2022-03-01T2300'\n",
    "decomFile     = open(decomFilename, 'rt') \n",
    "\n",
    "#read first line and print\n",
    "#firstLine = decomFile.readline();\n",
    "#print(firstLine)\n",
    "\n",
    "#the line appears to be json-formatted: pretty print json\n",
    "#firstLineJson = json.loads(firstLine)\n",
    "\n",
    "#read all lines of first file\n",
    "count = 0\n",
    "st    = time.time()\n",
    "for line in decomFile:\n",
    "    jsonline = json.loads(line)\n",
    "    #print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "    count = count + 1\n",
    "    if count > 10000: \n",
    "          break\n",
    "\n",
    "#print the last line\n",
    "print(json.dumps(jsonline, sort_keys=True, indent=4))\n",
    "\n",
    "#print the read duration\n",
    "dur         = round(time.time() - st,2)\n",
    "print(\"Loading took: \" + str(dur) + \" seconds\")\n",
    "print(\"The file had \" + str(count) + \"lines\")\n",
    "\n",
    "#finally close decomFile\n",
    "decomFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
